[
  {
    "objectID": "lab-06.html",
    "href": "lab-06.html",
    "title": "Lab 6: Timeseries Data",
    "section": "",
    "text": "In this lab, I downloaded stream flow data from the Cache la Poudre River (USGS site 06752260) and analyzed it using a few time series methods. I then predicted future stream flow data modeltime with respect to historic and future climate data.\nPackages Necessary:\n\nlibrary(tidyverse)\nlibrary(plotly)\n\nlibrary(dataRetrieval)\nlibrary(climateR) \nlibrary(terra)\nlibrary(exactextractr)\n\nlibrary(tidymodels)\n\n#timeseries packages\nlibrary(tsibble) \nlibrary(modeltime)\nlibrary(feasts)\nlibrary(timetk)\nlibrary(earth)\n\n\n\nGet the data using ‘dataRetrieval’, from January 1, 2013 to December 31, 2023, and summarize it into monthly averages.\n\nq_poudre &lt;- readNWISdv(siteNumber = \"06752260\",\n                       parameterCd = \"00060\",\n                       startDate = \"2013-01-01\",\n                       endDate = \"2023-12-31\") %&gt;%\n  renameNWISColumns(X_00060_00003 = 'Flow') %&gt;%\n  mutate(Date = yearmonth(Date)) %&gt;%\n  group_by(Date) %&gt;%\n  summarise(Flow = mean(Flow))\n\nDownload climate data from the GridMET and MACA data sets. The GridMET data set provides high-resolution climate data for the United States, while the MACA data set provides down-scaled climate data futures. These data sets are used to obtain climate data for the Cache la Poudre River Basin.\nTo start, we will use the findNLDI function to find the basin for the Cache la Poudre River. This function will return a list of features, including the basin polygon, which we can use to extract climate data.\n\nbasin &lt;- findNLDI(nwis = \"06752260\", find = \"basin\")\n\nmapview::mapview(basin$basin)\n\n\n\n\n\nWith an AOI defined, the climateR package is used to download climate data from the GridMET and MACA datasets. The code below downloads monthly climate data for the Cache la Poudre River Basin from January 1, 2013 to December 31, 2023. It then uses the exactextractr package to extract the climate data for the basin polygon. The code also converts the climate data into a tidy format, necessary for time series analysis and joins the data to the stream flow data.\n\nsdate &lt;- as.Date(\"2013-01-01\")\nedate &lt;- as.Date(\"2023-12-31\")\n\ngm &lt;- getTerraClim(AOI = basin$basin,             #gm=gridmet\n                   var = c(\"tmax\", \"ppt\", \"srad\"),\n                   startDate = sdate,\n                   endDate = edate) %&gt;%\n  unlist() %&gt;%\n  rast() %&gt;%\n  exact_extract(basin$basin, \"mean\", progress = FALSE)\n\nhistoric &lt;- mutate(gm, id = \"gridmet\") %&gt;%\n  pivot_longer(cols = -id) %&gt;%\n  mutate(name = sub(\"^mean\\\\.\", \"\", name)) %&gt;%\n  tidyr::extract(name, into = c(\"var\", \"index\"), \"(.*)_([^_]+$)\") %&gt;%\n  mutate(index = as.integer(index)) %&gt;%\n  mutate(Date = yearmonth(seq.Date(sdate, edate, by = \"month\")[as.numeric(index)])) %&gt;%\n  pivot_wider(id_cols = Date, names_from = var, values_from = value) %&gt;%\n  right_join(q_poudre, by = \"Date\")\n\nWe are interested in using exogenous climate data to predict future stream flow. Any time exogenous data is used to predict future values, it is required that we have values for the future. In other words, if we want to use precipitation (ppt) to predict stream flow in the future, we must have future ppt values.\nThe MACA data set is used to download future climate data for the Cache la Poudre River Basin from January 1, 2024 to December 31, 2033. The code below downloads the future climate data and converts it into a tidy format. The MACA data is used because it is developed by the same lab that created GridMET ensuring many of the same assumptions and methods are used.\nTwo quirks with MACA vs GridMET are that the units of temperature are in Kelvin and the names of the variables are different. The code below converts the temperature from Kelvin to Celsius and renames the variables to match those in the GridMET data set.\n\nsdate &lt;- as.Date(\"2024-01-01\")\nedate &lt;- as.Date(\"2033-12-31\")\n\nmaca &lt;- getMACA(AOI = basin$basin, \n                var = c(\"tasmax\", \"pr\", \"rsds\"), \n                timeRes = \"month\",\n                startDate = sdate,   \n                endDate = edate) %&gt;%\n  unlist() %&gt;%\n  rast() %&gt;%\n  exact_extract(basin$basin, \"mean\", progress = FALSE)\n\nfuture &lt;- mutate(maca, id = \"maca\") %&gt;% \n  pivot_longer(cols = -id) %&gt;%\n  mutate(name = sub(\"^mean\\\\.\", \"\", name)) %&gt;%\n  tidyr::extract(name, into = c(\"var\", \"index\"), \"(.*)_([^_]+$)\") %&gt;%\n  mutate(index = as.integer(index)) %&gt;%\n  mutate(Date = yearmonth(seq.Date(sdate, edate, by = \"month\")[as.numeric(index)])) %&gt;%\n  pivot_wider(id_cols = Date, names_from = var, values_from = value)\n\nnames(future) &lt;- c(\"Date\", \"ppt\", \"srad\", \"tmax\")\n\nfuture &lt;- mutate(future, tmax = tmax - 273.15)"
  },
  {
    "objectID": "lab-06.html#modeltime-prediction",
    "href": "lab-06.html#modeltime-prediction",
    "title": "Lab 6: Timeseries Data",
    "section": "Modeltime Prediction",
    "text": "Modeltime Prediction\n\nData Prep\nNow we are going to use the modeltime package to predict future streamflow using the climate data. We will use the modeltime package to create a time series model. First, we need to prep the data. In this case we want both the historic and future data to be in the same format where the date column is a Date object (name it date for ease) and the other columns are numeric. In both cases each need to be a tibble:\n\nhistoric_mod &lt;- historic %&gt;%\n  as_tibble() %&gt;%\n  mutate(date = as_date(Date),\n         month = factor(month(date))) %&gt;%\n  select(-Date)\n\nfuture_mod &lt;- future %&gt;%\n  as_tibble() %&gt;%\n  mutate(date = as_date(Date),\n         month = factor(month(date)))%&gt;%\n  select(-Date)\n\nWe will use the time_series_split() function to split the data into training and testing sets. The training set will be used to train the model and the testing set will be used to test the model. Remember in this case we are using a time series split, so the training set will be defined by a period rather then a percent of hold out data. In this case we will use a 24 month period testing set (assess). Make sure to set a seed and extract the training and testing data.frames with training() and testing().\n\nset.seed(329)\n\nsplits &lt;- time_series_split(historic_mod, date_var = date, assess = '24 months', cumulative = TRUE)\n\ntraining &lt;- training(splits)\ntesting &lt;- training(splits)\n\n\n\nModel Definition\nChose at least three models to test - with one being arima and one being prophet. Store these models (specification + engine) in a list.\n\nmods &lt;- list(\n  arima_reg() %&gt;%\n    set_engine(\"auto_arima\"),\n  \n  prophet_reg() %&gt;%\n    set_engine(\"prophet\"),\n  \n  prophet_boost() %&gt;%\n    set_engine(\"prophet_xgboost\"),\n  \n  exp_smoothing() %&gt;%\n    set_engine(engine = 'ets'),\n  \n  mars(mode = 'regression') %&gt;%\n    set_engine('earth')\n)\n\n\n\nModel Fitting\nNext, we will use the fit() function to fit the models to the training data. We will use the map() function from the purrr package to iterate over the list of models and fit each model to the training data. The fit() function takes a formula, a model specification, and a data.frame as arguments. Here you can build any formula you see fit. What components of the date object do you want to use (e.g. month? season?). What climate variables do you want to use?\nThe only requirement is that the response variable is Flow and the date variable is included as a predictor.\nOnce the models are fitted, we will use the as_modeltime_table() function to convert the list of fit models into a modeltime table. This will allow us to use the modeltime package to make predictions.\n\nmodels &lt;- map(mods, function(model_spec){\n  model_spec %&gt;% fit(Flow ~ date, data = training)\n})\n\nmod_tbl &lt;- as_modeltime_table(models)\n\n\n\nModel Calibration\nNext, we will use the modeltime_calibrate() function to calibrate the models. This function takes a modeltime table and a data.frame of testing data as arguments. The function will return a modeltime table with the calibrated models.\nThe results of the calibration can be passed to modeltime_accuracy() to assess the accuracy of the models.\nCalibrate the models using the testing data and assess the accuracy of the models describing what you see!\n\ncal_table &lt;- modeltime_calibrate(mod_tbl, testing, quiet = FALSE)\n\nmodeltime_accuracy(cal_table) %&gt;%\n  arrange(mae)\n\n# A tibble: 5 × 9\n  .model_id .model_desc             .type    mae  mape  mase smape  rmse    rsq\n      &lt;int&gt; &lt;chr&gt;                   &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1         4 ETS(M,N,M)              Fitted  131.  181. 0.597  61.7  232. 0.720 \n2         1 ARIMA(1,0,0)(0,1,0)[12] Fitted  132.  102. 0.600  57.8  277. 0.640 \n3         2 PROPHET                 Fitted  144.  176. 0.655  80.4  269. 0.619 \n4         3 PROPHET                 Fitted  144.  176. 0.655  80.4  269. 0.619 \n5         5 EARTH                   Test    268.  460. 1.22  108.   428. 0.0410\n\n\n\n\nForecasting\nWith a calibrated model set in place, we can now make predictions using the modeltime_forecast(). Because we are using exogenous data, we need to pass the actual data to the function. This is because the model needs to know what the actual values are in order to make predictions.\nAs a first step, lets use the calibrated models to make predictions on the testing data. Here you will need to specify the actual_data (historic data tibble) and the new_data (testing).\nThe outputs can be passed to the plot_modeltime_forecast() function to visualize the predictions.\n\nfc &lt;- cal_table %&gt;%\n  modeltime_forecast(h = '24 months',\n                     new_data = testing,\n                     actual_data = historic_mod)\n\nplot_modeltime_forecast(fc)\n\n\n\n\n\n\n\nRefitting the Model\nNow that we have a calibrated model set, we can refit the models to the full dataset. This is important because the models are only as good as the data they are trained on. By refitting the models to the full dataset, we can improve the accuracy of the predictions.\nTo do this, we will use the modeltime_refit() function. This function takes a calibrataion table and the full historic tibble as the data argument. The function will return a modeltime table with the refitted models. Like before, the accuracy of the models can be assessed using the modeltime_accuracy() function.\n\nrefit_tbl &lt;- cal_table %&gt;%\n  modeltime_refit(data = historic_mod)\n\nmodeltime_accuracy(refit_tbl) %&gt;%\n  arrange(mae)\n\n# A tibble: 5 × 9\n  .model_id .model_desc               .type   mae  mape  mase smape  rmse    rsq\n      &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1         4 ETS(M,N,M)                Fitt…  131.  181. 0.597  61.7  232. 0.720 \n2         1 UPDATE: ARIMA(0,0,2)(0,1… Fitt…  132.  102. 0.600  57.8  277. 0.640 \n3         2 PROPHET                   Fitt…  144.  176. 0.655  80.4  269. 0.619 \n4         3 PROPHET                   Fitt…  144.  176. 0.655  80.4  269. 0.619 \n5         5 EARTH                     Test   268.  460. 1.22  108.   428. 0.0410\n\n\n\n\nLooking into the future\nNow we are at the end! We can use the refitted models to make predictions on the future data. This is where we will use the future tibble we created earlier as the new_data. The actual_data argument will be the historic data tibble.\nUsing your refitted models, the modeltime_forecast() function will return a modeltime table with the predictions that can be passed to the plot_modeltime_forecast() function to visualize the predictions.\n\nrefit_tbl %&gt;%\n  modeltime_forecast(h = '10 years',\n                     new_data = future_mod,\n                     actual_data = historic_mod) %&gt;%\n  plot_modeltime_forecast()\n\n\n\n\n\n\n\nWrap up\nLooking at your predictions what do you think? How do the models compare? What do you think the future streamflow will be? What are the limitations of this analysis? What are the assumptions of the models?\nThe models do capture the monthly variation of streamflow peaking in June. There is an overall negative trend, predicting decreased streamflow from the Poudre River over 10 years. The limitation here is that only the historical discharge data has been taken into account, not temperature, precipitation, aridity, water rights, or any other potential predictors that may affect true streamflow in the future. The model assumes the historical trend will continue, and that the system is mostly stationary. Climate is non-stationary of course and these values are limited. The model also predicts the trend will continue, even though there may be extreme highs or lows in the near future based on precipitation patterns.\nPerhaps you see negative values in the predictions; perhaps you don’t see strong agreement between the future starting points around Jan 2024; perhaps you see other patterns that seem systemic and questionably wrong. At this stage that is quite alright. In this simple lab, we did not worry about issue like cross validation, feature engineering, or hyperparameter tuning. These are all important steps in the modeling process and should be considered in a real world application."
  },
  {
    "objectID": "lab-04.html",
    "href": "lab-04.html",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "",
    "text": "Flooding"
  },
  {
    "objectID": "lab-04.html#step-1-aoi-identification",
    "href": "lab-04.html#step-1-aoi-identification",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 1: AOI Identification",
    "text": "Step 1: AOI Identification\nExtract the flood extents for Palo, Iowa and its surroundings. To do this, use the geocoding capabilities within the AOI package.\nThis region defines the AOI for this analysis.\n\npalo &lt;- AOI::geocode('Palo, Iowa', bbox = TRUE)"
  },
  {
    "objectID": "lab-04.html#step-2-temporal-identification",
    "href": "lab-04.html#step-2-temporal-identification",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 2: Temporal Identification",
    "text": "Step 2: Temporal Identification\nThe flood event occurred on September 26, 2016. A primary challenge with remote sensing is the fact that all satellite imagery is not available at all times. In this case Landsat 8 has an 8 day revisit time. To ensure an image within the date of the flood is captured, set the time range to the period between September 24th - 29th of 2016. Use the form YYYY-MM-DD/YYYY-MM-DD.\n\ntemporal_range &lt;- \"2016-09-24/2016-09-29\""
  },
  {
    "objectID": "lab-04.html#step-3-identifying-the-relevant-images",
    "href": "lab-04.html#step-3-identifying-the-relevant-images",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 3: Identifying the relevant Images",
    "text": "Step 3: Identifying the relevant Images\nThe next step is to identify the images that are available for our AOI and time range. This is where the rstac package comes in. The rstac package provides a simple interface to the SpatioTemporal Asset Catalog (STAC) API, which is a standard for discovering and accessing geospatial data.\nSTAC is a specification for describing geospatial data in a consistent way, making it easier to discover and access datasets. It provides a standardized way to describe the metadata of geospatial assets, including their spatial and temporal extents, data formats, and other relevant information.\n\nCatalog: A catalog is a collection of STAC items and collections. It serves as a top-level container for organizing and managing geospatial data. A catalog can contain multiple collections, each representing a specific dataset or group of related datasets.\nItems: The basic unit of data in STAC. Each item represents a single asset, such as a satellite image or a vector dataset. Items contain metadata that describes the asset, including its spatial and temporal extents, data format, and other relevant information.\nAsset: An asset is a specific file or data product associated with an item. For example, a single satellite image may have multiple assets, such as different bands or processing levels. Assets are typically stored in a cloud storage system and can be accessed via URLs.\n\nFor this project I use a STAC catalog to identify the data available for the analysis. I want data from the Landsat 8 collection which is served by the USGS (via AWS), Google, and Microsoft Planetary Computer (MPC). MPC is a free and open access source.\nOpen a connection to this endpoint with the stac function:\n\n(stac_query &lt;- stac('https://planetarycomputer.microsoft.com/api/stac/v1'))\n\n###rstac_query\n- url: https://planetarycomputer.microsoft.com/api/stac/v1/\n- params:\n- field(s): version, base_url, endpoint, params, verb, encode\n\n\nThis connection will provide an open entry to ALL data hosted by MPC. The stac_search function allows us to reduce the catalog to assets that match certain criteria (just like dplyr::filter reduces a data.frame). The get_request() function sends your search to the STAC API returning the metadata about the objects that match a criteria. The service implementation at MPC sets a return limit of 250 items (but it could be overridden with the limit parameter).\n\n(stac_query &lt;- stac(\"https://planetarycomputer.microsoft.com/api/stac/v1\") %&gt;%\n  stac_search(\n    collections = \"landsat-c2-l2\",\n    datetime    = temporal_range,\n    bbox        = sf::st_bbox(palo),\n    limit = 1) %&gt;%\n  get_request()) %&gt;%\n  items_sign(sign_planetary_computer())"
  },
  {
    "objectID": "lab-04.html#step-4.-downloading-the-needed-images",
    "href": "lab-04.html#step-4.-downloading-the-needed-images",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 4. Downloading the needed Images",
    "text": "Step 4. Downloading the needed Images\nAfter identifying the item(s) we want, it is ready to download using assets_download(). In total, a Landsat 8 item has 11 bands, but I only want to focus on 6.\n\nassets_download(items = stac_query,\n                asset_names = bands,\n                output_dir = 'data',\n                overwrite = TRUE)\n\n##my computer was not downloading the files correctly. I instead used a copy from Mike Johnson.\n\nWith a set of local files, a raster object can be created! The files need to be in the order of the bands.\n\nThe list.files() can search a directory for a pattern and return a list of files. The recursive argument will search all sub-directories. The full.names argument will return the full path to the files.\nThe rast() function will read the files into a raster object.\nThe setNames() function will set the names of the bands to the names we defined above.\n\n\nbands &lt;- c('coastal', 'blue', 'green', 'red', 'nir08', 'swir16')\n\nfiles &lt;- list.files('C:/Users/horre/Desktop/csu_523c/data/landsat-c2/', recursive = T, full.names = T)\n\nr_palo &lt;- rast(files)\nr_palo &lt;- setNames(r_palo, bands)"
  },
  {
    "objectID": "lab-04.html#step-1.1-aalyze-the-images",
    "href": "lab-04.html#step-1.1-aalyze-the-images",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 1.1 Aalyze the Images",
    "text": "Step 1.1 Aalyze the Images\nOnly analyze the image for the regions surrounding Palo (our AOI). Transform the AOI to the CRS of the landsat stack and use it to crop the raster stack.\n\npalo &lt;- st_transform(palo, crs(r_palo))\n\nsame.crs(palo, r_palo)\n\n[1] TRUE\n\nr_palo &lt;- crop(r_palo, palo)\n\nAwesome! I have now (1) identified, (2) downloaded, and (3) saved the images.\nI have loaded them as a multiband SpatRast object and cropped the domain to the AOI. Now make a few RGB plots to see what these images reveal. ****"
  },
  {
    "objectID": "lab-04.html#step-3.1-raster-algebra",
    "href": "lab-04.html#step-3.1-raster-algebra",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 3.1 Raster Algebra",
    "text": "Step 3.1 Raster Algebra\n\nCreate 5 new rasters using the formulas for NDVI, NDWI, MNDWI, WRI, and SWI\nCombine those new rasters into a stacked object (c())\nSet the names of your new stack to useful values\nPlot the new stack, using the following palette: (colorRampPalette(c(‘blue’, ‘white’, ‘red’))(256))\n\n\nr_palo_ndvi &lt;- (r_palo$nir08 - r_palo$red)/(r_palo$nir08 + r_palo$red)\nr_palo_ndwi &lt;- (r_palo$green - r_palo$nir08)/(r_palo$green + r_palo$nir08)\nr_palo_mndwi &lt;- (r_palo$green - r_palo$swir16)/(r_palo$green + r_palo$swir16)\nr_palo_wri &lt;- (r_palo$green + r_palo$red)/(r_palo$nir08 + r_palo$swir16)\nr_palo_swi &lt;- 1/(sqrt((r_palo$blue)-(r_palo$swir16)))\n\nr_palo_indices &lt;- c(r_palo_ndvi, r_palo_ndwi, r_palo_mndwi, r_palo_wri, r_palo_swi)\nindices &lt;- c('ndvi', 'ndwi', 'mndwi', 'wri', 'swi')\n\nr_palo_indices &lt;- setNames(r_palo_indices, indices)\n\ncolor &lt;- colorRampPalette(c('blue', 'white', 'red'))(256)\n\nplot(r_palo_indices, col = color)\n\n\n\n\n\n\n\n\nDescribe the 5 images. How are they similar and where do they deviate?\nThey all highlight the difference between the river and other features. Some indices show the river and water in blue (NDVI and SWI) while the others show the river and water features as red NDVI and NDWI help differentiate water and vegetation it looks like. The SWI takes away all features BUT water. They all help to differentiate the three main surface features, water, vegetation, and dry surface."
  },
  {
    "objectID": "lab-04.html#step-3.2-raster-thresholding",
    "href": "lab-04.html#step-3.2-raster-thresholding",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 3.2 Raster Thresholding",
    "text": "Step 3.2 Raster Thresholding\nExtract the flood extents from each of the above rasters using the thresholds defined in the index table above.\nUse the app function and apply a custom formula for each calculated field from step 1 that applies the threshold in a way that flooded cells are 1 and non-flooded cells are 0.\nThe app function applies a function to each cell of the raster, and the ifelse function is used to set the values based on the threshold.\nFor all 5 index rasters do the following apply the appropriate threshold and then do the following:\n\nStack the binary ([0,1]) files into a new stack (c()),\nSet the names to meaningful descriptions (setNames)\nPerform one more classifier (app) making sure that all NA values are set to zero.\nPlot the stack so that floods are blue, and background is white.\n\n\nthreshold0 &lt;- function(x){\n  ifelse(x &lt; 0, 1, 0)\n}\n\nthreshold1 &lt;- function(x){\n  ifelse(x &gt; 0, 1, 0)\n}\n\nthreshold11 &lt;- function(x){\n  ifelse(x &gt; 1, 1, 0)\n}\n\nthreshold5 &lt;- function(x){\n  ifelse(x &lt; 5, 1, 0)\n}\n\nr_palo_ndvif &lt;- app(r_palo_ndvi, threshold0)\nr_palo_ndwif &lt;- app(r_palo_ndwi, threshold1)\nr_palo_mndwif &lt;- app(r_palo_mndwi, threshold1)\nr_palo_wrif &lt;- app(r_palo_wri, threshold11)\nr_palo_swif &lt;- app(r_palo_swi, threshold5)\n\nr_palo_flood &lt;- c(r_palo_ndvif, r_palo_ndwif, r_palo_mndwif, r_palo_wrif, r_palo_swif)\nr_palo_flood &lt;- app(r_palo_flood, fun = function(x) ifelse(is.na(x), 0, x))\n\nplot(r_palo_flood, col = c('white', 'blue'))"
  },
  {
    "objectID": "lab-04.html#step-3.3",
    "href": "lab-04.html#step-3.3",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 3.3",
    "text": "Step 3.3\nDescribe the differences and similarities between the maps.\n\nThey all capture a similar amount of water. It appears that MNDWI and WRI capture the most flood water based on the thresholds."
  },
  {
    "objectID": "lab-04.html#step-4.1",
    "href": "lab-04.html#step-4.1",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 4.1",
    "text": "Step 4.1\nTo produce a consistent/reproducible result from a random process in R, set a seed. Do so using set.seed.\n\nset.seed(7228)"
  },
  {
    "objectID": "lab-04.html#step-4.2",
    "href": "lab-04.html#step-4.2",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 4.2",
    "text": "Step 4.2\n\nExtract the values from the 6-band raster stack with values\nCheck the dimensions of the extracted values with dim. What do the dimensions of the extracted values tell you about how the data was extracted?\nthe data was extracted in columns and rows, a data frame and it keeps the same properties and dimensions as the original raster\nRemove NA values from your extracted data with na.omit for safety\n\n\nv &lt;- values(r_palo_indices)\nclass(v)\n\n[1] \"matrix\" \"array\" \n\ndim(v)\n\n[1] 12192     5\n\nnrow(v)\n\n[1] 12192\n\nncol(v)\n\n[1] 5\n\nlength(v)\n\n[1] 60960\n\nres(r_palo_indices)\n\n[1] 30 30\n\nv &lt;- values(r_palo_indices)\nidx &lt;- which(!apply(is.na(v), 1, any))\nv &lt;- na.omit(v)"
  },
  {
    "objectID": "lab-04.html#step-4.3",
    "href": "lab-04.html#step-4.3",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 4.3",
    "text": "Step 4.3\n\nUse the kmeans clustering algorithm from the stats package to cluster the extracted raster data to a specified number of clusters k (centers). Start with 12.\nOnce the kmeans algorithm runs, the output will be a list of components. One of these is cluster which provides a vector of integers from (1:k) indicating the cluster to which each row was allocated.\n\n\npalo_kmeans12 &lt;- kmeans(v, centers = 12, iter.max = 100)\npalo_kmeans12$cluster\n\n  [1]  2  4  4 12 12 12 12  2  2  2  4  9  9  9  2 12  4  2  2  5  3  3 10  9  2\n [26]  2  3  3  3 11  8  2  2  2  3  3  3  3  5 12  2  2  2  5  3  3  3  3  3  2\n [51]  2  2  2  3  3  3  3  3  3  2  2  2  2 11  9  4  6  2  2  2  2 12  9  4  5\n [76]  6  8  2  2  2  2 12  3  9  2  6  4  4  4  5  2  2  2  2  4  4  9  9  9  5\n[101]  9  9  2  2 12  2  9  2  2  2  2  5  9  9  6  6  5  4 12  6  4  2  6 11  2\n[126]  2  2  2  9  2  2  2  2  9  9  2  2  2  2 10  6  9  9  2  2  2  2  4  9  9\n[151]  9 11  2  2  2  2  6  9  9  9 11 11  2  2  2  2  8  9  9  9  9  2  2  2 12\n[176]  9  9  6  5  2  2  2  2  2  2 10  2  2  2 12  6 11 12  2  2  2  2  5 12  2\n[201]  2  2  2 12  2  2  2  2  2 12  2  2  2  2  2  9  8  9  2  2  2  2 10  5  6\n[226] 12  2  2  2  2 12  8  6  5  2  2  2  2  2  7  9  5  2  2  2  2 12  7 12  2\n[251]  2  2  2 10 10  6  2  2  2  2  4 12 11  6  2  2  2  2 12  4  9  2  6  6  6\n[276]  5 12  2  2  2  5  4  6  6  6  4  2  2  2  2  7 11  9  9 12  5 12  2  2  2\n[301]  2  6  5  7 12  2  2  2  2 12  8 12  2  2  2  2  2  5  6  5  6  6  6  2  2\n[326]  2  2  2  2 10  5  2  2  2  2  2  2  2 10  2  2  2  2  2  2 12 12  2  2  2\n[351]  2  2 12 12  2  2  2  2  2 12  4  2  2  2  2  6  4  2  2  2  2  2  1 12  2\n[376]  2  2  2 12  5  2  2  2  2  6  6  6  6  2  2 12  6  6  9 10 12  6  9  9  7\n[401]  9 10  9  9  9 11  2  2  2 12  5  2 12  5  1  2  2  2  2 12 10  2 12 10 11\n[426] 11  2  2  2  2  6  5  6  6  6  6  6  2  9  9  6  6  6  6  4  6  2  2  2  2\n[451]  4  9  6  9  9  9  9  9  9  9  9  9  9  9  9  4  6  2  2  2  2  2  5  6  6\n[476]  4  5 12  2  2  9  9  9  9  6  2  2  2  2 10  4  4 11 10  6 12  9  9  9  6\n[501]  5  2  2  2  2 12 12  2  2  2  4  4  6  9  9  9 12  2  2  2  2 12  4 12  2\n[526] 10  6  2  2  2  2 10  8  8 11  2  2  2  2  8  2  2  2  2  2  2  2  2  2  2\n[551] 12  1  7  2  2  2  8  2  2  2 12  9  6  5  2  2  2 12 10  2  2  2 12  2  2\n[576]  2  2  2  2  2  2  2 10  2  2  2  2  2  9  9 12 12  2  2  2  2  2  2  2  2\n[601]  2 10  7 12  2  2  2  2  2  2  2  2  9  9  6  7  2  2  2  2  2  2  2  2  2\n[626]  2  2  2  2  8 12  4 12  9  2  2  2  2  2  2  2  2  9 10 12 11  2  2  2  2\n[651]  2  2  2  2  2  2 12 12 11 12  9  9  2  2  2  2  2  2  2 11  2  5  5 12  2\n[676]  2  2  2  2  2 11 12 12  2  2  2 11 12  1  2  2  5  2  8  2  3  5  2  2  5\n[701] 12  4  4  4  5 11  6  9  6  7 12  2  4  2  6  5 11  2  2  4  2  9  6 10  2\n[726]  2  2  7  6  9  2  2 12  2  2  2  2  6  6  6  2  2 10  2  2  2  2  2  2  6\n[751]  6  6  2  2  5  2  2  2  2  2  2 12  9  6  2  2  6  2  2  2  6 10 10  9 11\n[776]  6  2 10  4  2  2 11  9  6  6  2  2  4  9  5  6  4  6  2  2  4  2  2  2  2\n[801]  2  2  2  2  2  6  2  2  2  2 10 12  2  2  2  2 12  2 12 10  2  2 12  2  2\n[826] 12  2  6  2  4  2  9  4  2  2 12 12  2  2  2  5  2  9  2  2  2 10  4  9  2\n[851]  2  2  2  2  2  2  2  5  2  2  2  2  2  2 10  2  9 12  6\n\npalo_kmeans8 &lt;- kmeans(v, centers = 8, iter.max = 100)\npalo_kmeans8$cluster\n\n  [1] 7 5 2 5 5 5 5 7 7 7 2 1 1 1 7 5 5 7 7 2 5 5 8 1 7 7 5 1 5 4 8 7 7 7 5 1 1\n [38] 5 2 5 7 7 7 2 5 5 5 1 5 7 7 7 7 1 1 5 5 5 1 7 7 7 7 4 1 5 5 7 7 7 7 7 1 5\n [75] 2 7 8 7 7 7 7 5 5 1 7 5 5 2 5 2 7 7 7 7 5 2 1 1 1 2 1 1 7 7 5 7 1 7 7 7 7\n[112] 2 1 1 1 5 2 5 5 5 5 7 7 4 7 7 7 7 1 7 7 7 7 1 1 7 7 7 7 2 1 1 1 7 7 7 7 5\n[149] 1 1 1 4 7 7 7 7 1 1 1 1 4 4 7 7 7 7 8 1 1 1 1 7 7 7 5 1 1 5 2 7 7 7 7 7 7\n[186] 8 7 7 7 5 5 4 5 7 7 7 7 2 5 7 7 7 7 5 7 7 7 7 7 5 7 7 7 7 7 1 8 1 7 7 7 7\n[223] 2 2 1 5 7 7 7 7 5 8 7 2 7 7 7 7 7 3 1 2 7 7 7 7 7 3 5 7 7 7 7 8 8 7 7 7 7\n[260] 7 2 7 4 7 7 7 7 7 5 5 1 7 7 7 1 2 5 7 7 7 2 5 7 7 1 5 7 7 7 7 3 4 1 1 5 2\n[297] 5 7 7 7 7 5 2 3 5 7 7 7 7 5 8 5 7 7 7 7 7 2 1 2 5 5 5 7 7 7 7 7 7 8 2 7 7\n[334] 7 7 7 7 7 8 7 7 7 7 7 7 5 5 7 7 7 7 7 5 5 7 7 7 7 7 5 2 7 7 7 7 7 5 7 7 7\n[371] 7 7 6 5 7 7 7 7 5 2 7 7 7 7 5 1 5 1 7 7 5 1 1 1 2 5 1 1 1 3 1 2 1 1 1 4 7\n[408] 7 7 5 2 7 5 2 6 7 7 7 7 5 2 7 7 2 4 4 7 7 7 7 5 2 5 1 1 5 7 7 1 1 1 5 5 1\n[445] 5 7 7 7 7 7 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 5 7 7 7 7 7 2 5 5 5 2 5 7 7 1\n[482] 1 1 1 7 7 7 7 7 2 5 5 4 2 7 5 1 1 1 7 2 7 7 7 7 7 5 7 7 7 5 5 7 1 1 1 5 7\n[519] 7 7 7 5 2 5 7 2 5 7 7 7 7 8 8 8 4 7 7 7 7 8 7 7 7 7 7 7 7 7 7 7 5 6 3 7 7\n[556] 7 8 7 7 7 5 1 5 2 7 7 7 5 2 7 7 7 7 7 7 7 7 7 7 7 7 7 2 7 7 7 7 7 1 1 5 5\n[593] 7 7 7 7 7 7 7 7 7 8 3 5 7 7 7 7 7 7 7 7 7 1 5 3 7 7 7 7 7 7 7 7 7 7 7 7 7\n[630] 8 5 5 5 1 7 7 7 7 7 7 7 7 1 8 5 4 7 7 7 7 7 7 7 7 7 7 5 5 4 5 1 7 7 7 7 7\n[667] 7 7 7 4 7 2 2 5 7 7 7 7 7 7 4 5 5 7 7 7 4 5 6 7 7 2 7 8 7 5 2 7 7 2 5 5 5\n[704] 5 2 4 1 1 1 3 5 7 2 7 7 2 4 7 7 5 1 1 5 8 7 7 7 3 7 1 1 7 5 7 7 7 7 7 7 7\n[741] 7 7 2 7 7 7 7 7 7 5 7 7 7 7 2 7 7 7 7 7 7 5 1 7 7 7 7 7 7 7 7 2 2 1 4 7 7\n[778] 2 5 7 7 4 1 7 5 7 7 2 1 2 7 5 7 7 7 5 7 7 7 7 7 7 7 7 7 1 7 7 7 7 2 5 7 7\n[815] 7 7 5 7 5 2 7 7 5 7 7 7 7 5 7 5 7 1 5 7 7 7 5 7 7 7 2 7 1 7 7 7 2 5 1 7 7\n[852] 7 7 7 7 7 7 2 7 7 7 7 7 7 2 7 1 5 7\n\npalo_kmeans5 &lt;- kmeans(v, centers = 5, iter.max = 100)\npalo_kmeans5$cluster\n\n  [1] 4 5 5 5 4 5 4 4 4 4 5 2 2 2 4 5 5 4 4 5 2 5 1 2 4 4 5 2 2 1 1 4 4 4 5 2 2\n [38] 2 5 5 4 4 4 5 5 2 2 2 5 4 4 4 4 2 2 2 2 4 2 4 4 4 4 1 2 5 5 4 4 4 4 4 2 5\n [75] 5 4 1 4 4 4 4 5 2 2 4 4 5 5 5 1 4 4 4 4 5 5 2 2 2 5 2 2 4 4 5 4 2 4 4 4 4\n[112] 1 2 2 2 4 5 5 5 4 5 4 4 1 4 4 4 4 2 4 4 4 4 2 2 4 4 4 4 5 2 2 2 4 4 4 4 5\n[149] 2 2 2 1 4 4 4 4 2 2 2 2 1 1 4 4 4 4 1 2 2 2 2 4 4 4 5 2 2 4 5 4 4 4 4 4 4\n[186] 5 4 4 4 5 5 1 5 4 4 4 4 5 5 4 4 4 4 5 4 4 4 4 4 4 4 4 4 4 4 2 1 2 4 4 4 4\n[223] 5 5 2 5 4 4 4 4 4 1 4 5 4 4 4 4 4 1 2 5 4 4 4 4 4 3 4 4 4 4 4 1 5 4 4 4 4\n[260] 4 5 4 1 4 4 4 4 4 5 5 2 4 4 4 2 5 4 4 4 4 5 5 4 4 2 5 4 4 4 4 1 1 2 2 5 5\n[297] 4 4 4 4 4 4 5 3 4 4 4 4 4 4 1 4 4 4 4 4 4 5 2 5 4 4 2 4 4 4 4 4 4 5 5 4 4\n[334] 4 4 4 4 4 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 5 4 4 4\n[371] 4 4 3 5 4 4 4 4 5 5 4 4 4 4 5 2 2 2 4 4 4 2 2 2 5 4 2 2 2 1 2 5 2 2 2 1 4\n[408] 4 4 4 5 4 5 5 3 2 4 4 4 5 5 4 4 5 1 1 4 4 4 4 4 5 5 2 2 2 4 4 2 2 2 4 4 2\n[445] 5 4 4 4 4 4 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 4 4 4 4 4 4 5 2 4 5 5 5 4 4 2\n[482] 2 2 2 4 4 4 4 4 5 5 5 1 5 4 5 2 2 2 4 5 4 4 4 4 4 5 4 4 4 5 5 4 2 2 2 5 4\n[519] 4 4 4 5 5 5 4 5 4 4 4 4 4 1 1 1 1 4 4 4 4 1 4 4 4 4 4 4 4 4 4 4 5 3 3 4 4\n[556] 4 1 4 4 4 5 2 5 5 4 4 4 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 2 2 5 5\n[593] 4 4 4 4 4 4 4 4 4 5 3 4 4 4 4 4 4 4 4 4 2 2 4 1 4 4 4 4 4 4 4 4 4 4 4 4 4\n[630] 1 5 5 4 2 4 4 4 4 4 4 4 4 2 5 5 1 4 4 4 4 4 4 4 4 4 4 4 4 1 5 2 4 4 4 4 4\n[667] 4 4 4 1 4 5 1 4 4 4 4 4 4 4 1 4 5 4 4 4 1 4 3 4 4 1 4 1 4 5 5 4 4 1 5 5 5\n[704] 5 5 1 2 2 2 3 5 4 5 4 4 1 1 4 4 5 2 2 5 1 4 4 4 3 4 2 2 4 4 4 4 4 4 4 4 4\n[741] 4 4 5 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 4 2 4 4 4 4 4 4 4 4 5 5 2 1 4 4\n[778] 5 5 4 4 1 2 4 4 4 4 5 2 5 4 5 4 4 4 5 4 4 4 4 4 4 4 4 4 2 4 4 4 4 5 4 4 4\n[815] 4 4 5 4 5 5 4 4 4 4 4 4 4 4 4 5 4 2 5 4 4 4 5 4 4 4 5 4 2 4 4 4 5 5 2 4 4\n[852] 4 4 4 4 4 4 5 4 4 4 4 4 4 5 4 2 5 4\n\npalo_kmeans20 &lt;- kmeans(v, centers = 20, iter.max = 100)\npalo_kmeans20$cluster\n\n  [1]  2 17 20 16 16 16 17 15  8 15 20  2 12 12  2 16 20  8  8 10  6  6 13 12  8\n [26]  8  6  6  6 14  4  8  8  2  6  6  6  6 10 20  8  8  5 10  1  6  6  6  1 15\n [51]  8  8 15  6  6  6  6  6  6 15  8  8 15 14  9  1  1  8  8  8  8 15 19  1 10\n [76] 18  4  8  8  8 15 17  1  7 18 16  1 20 20 10  8  8  8 15 20  1 12 12 19 10\n[101] 12 19  5  5 16 18  2  5  8  8 15 10  7 19  9 18 10  1 16 16 17 15 18 14  5\n[126]  8  8 15  7  8  8  8 15 12 12  5  8  8 15 20  9 12 12  8  8  8  8 20 12 12\n[151] 12 14  8  8  8  8  9 12 12 12 14 14  8  8  8 15  4 19 12 12 12  8  8  8 17\n[176]  7 19 16 10  8  8  5  8  8  8 13  8  8  8 17  1 14 17  8  8  8  8 10 17  2\n[201]  8  8  8 17 15  2  8  8  8 16  8  2  8  8 15  9  4  2  2  2  8  8 13 10  9\n[226] 17  2  8  8  8 17  4 18 10  8  8  8  8  8 11 19 10  8  8  8  8 15 11 17  8\n[251]  8  8  5 13 13 18 15  5  8  5 20 15 14 18  5  5  5 15 17  1 19 18 18 18  9\n[276] 10 16  5  5 15 10  1 18 18 18  1  5  5  5 15 11 14  7 19 17 10 16  5  8  5\n[301] 15 16 10 11 16  5  5  5 15 16  4 16  5  5  5  5 15 10  9 10 16 16 18  8  5\n[326]  5  5  5  5 13 10  5  5  5  5  5 15 15 13  5  5  5  5  5 15 16 16  5  5  5\n[351]  5 15 16 16  5  5  5 18  5 16 20  5  5  5  5 18  1  5  5  5  5  5  3 17  5\n[376]  5  5  5 17 10  5  5  5  5 16  9  9  9 18  5 16  9  9 19 20 17  9  9  9 11\n[401]  7 20  9  7 19 14  5  8  8 15 10 18 17 10  3 19  5  8  5 17 13 15 15 13 14\n[426] 14  5  5  8 15  1 10 16 18  9  9 18  5 19  9  9 16 16  9  1 18  5  5  8  5\n[451]  1  7 19  9 19 19 12 12 12 12 12 19 19 19 19 20 16 18  5  8  8 15 10  9 18\n[476] 17 10 16  5  5 12 19 19 19 18  5  5  8  8 13 17  1 14 13 18 17 19 12 12 18\n[501] 10 18  5  8  8 15 17  5  8  8 20 20 18 19 12 12 17  5  5  8  8 17 20 17 15\n[526] 13 18  5  8  8  8 13  4  4 14  5  8  8 15  4  5  5  8 15  2  8  5  5  5  5\n[551] 17  3 11  5  8 15  4  5  8  8 17 19  1 10  5  8  8 17 13  5  8  2 15  8  8\n[576]  2 18  5  2  8  5 15 20  5  8  8  5  5  2 12 17 16  2  8  8  8  8  8  5  8\n[601] 15 13 11 16  8  2  8  8  8  8  8  8  2 12 16 11  5  2  2  2  8  8  8  8  8\n[626]  8  8  8 15  4 16  1 16 12  2  8  8  8  8  8  8  2  2 13 17 14 15  8  8  8\n[651]  8  8  8  8  8  2 16 16 14 17 12  2  2  2  8  8  8  8  8 14  5 10 10 17  2\n[676]  2  2  8  8  8 14 16 17  2  8  8 14 16  3  2  8 10 15  4  8  1 10 18  5 10\n[701] 17  1 20  1 10 14  9  9 18 11 17 15 20  5 18 10 14  5  2  1 19 19  1 13  5\n[726]  2  8 11 18 19 19  2 16  8  8  2  8 18 18 18  5  5 20  5  5  8  8  8  5 16\n[751] 18 18  5  5 10  8  2  5  5  8  2 17 19 18  5  2 18  5  2  2 18 20 13 12 14\n[776] 18  5 13 20  5  5 14 19 18 16  5  5 20  7 10 18  1 18  5  8  1  5  5  2 15\n[801]  8  5  5  5  5  9  5  5  5  5 20 17  5 15  5  8 17  8 17 13  8 15 15  8 15\n[826] 15  5 18 15  1  5  2 20  5  5 15 17  2  5  5 20  8  2  8  5  5 13  1 12  5\n[851]  5  5 15  2  2  2  8 10  5  2  5  5  2  2 20  2 12 17 18"
  },
  {
    "objectID": "lab-04.html#step-4.4",
    "href": "lab-04.html#step-4.4",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 4.4",
    "text": "Step 4.4\n\nCreate a new raster object by copying one of the original bands.\nSet the values of the copied raster to the cluster vector from the output kmeans object.\nTry a few different clusters (k) to see how the map changes.\n\n\nclus_rast_ndvi &lt;- r_palo_indices$ndvi\nvalues(clus_rast_ndvi) &lt;- NA\nclus_rast_ndvi[idx] &lt;- palo_kmeans12$cluster\nplot(clus_rast_ndvi)\n\n\n\n\n\n\n\nclus_rast_ndvi &lt;- r_palo_indices$ndvi\nvalues(clus_rast_ndvi) &lt;- NA\nclus_rast_ndvi[idx] &lt;- palo_kmeans8$cluster\nplot(clus_rast_ndvi)"
  },
  {
    "objectID": "lab-04.html#step-4.5",
    "href": "lab-04.html#step-4.5",
    "title": "Lab 4: Palo, Iowa Flooding",
    "section": "Step 4.5",
    "text": "Step 4.5\nGreat! You now have a categorical raster with categories 1:k. The issue is we don’t know the value that corresponds to the flood water. To identify the flood category programatically, generate a table crossing the values of one of your binary flood rasters, with the values of your kmeans_raster. To do this, you will use the table function and pass it the values from a binary flood raster, and the values from your kmeans_raster. Here the following occurs:\n\ntable builds a contingency table counting the number of times each combination of factor levels in the input vector(s) occurs. This will give us a table quantifying how many cells with a value 1 are aligned with each of the k classes, and how many cells with a value 0 are aligned with each of the k classes. If you pass the binary flood values as the first argument to table then the unique values (0,1) will be the rows. They will always be sorted meaning you know the flooded cells will be in the second row.\nwhich.max() returns the index of the maximum value in a vector.\ncombine this information to identify the cluster in the kmeans data that coincides with the most flooded cells in the binary mask.\nOnce you know this value, use app to extract the flood mask in a similar way to the thresholding you did above.\nFinally add this to add to your flood raster stack with c() and make a new plot!"
  },
  {
    "objectID": "lab-02.html",
    "href": "lab-02.html",
    "title": "Lab 2: Distances and Projections",
    "section": "",
    "text": "Cities Data\n\n\nIn this lab I explored the properties of sf, sfc, and sfg features & objects; how they are stored; and issues related to distance calculation and coordinate transformation.\nI continue to build on my data wrangling and visualization skills; as well as document preparation via Quarto and GitHub.\n\n\nLibraries\n\n#Data Manipulation\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(units)\nlibrary(flextable)\n\n#Data Loading\nlibrary(USAboundaries)\nlibrary(rnaturalearth)\n\n# Visualization\nlibrary(gghighlight)\nlibrary(ggrepel)\nlibrary(knitr)\n\n\n\n\nBackground\nIn this lab, 4 main skills are covered:\n\nIngesting / building sf objects from R packages and CSVs. (Q1)\nManipulating geometries and coordinate systems (Q2)\nCalculating distances (Q2)\nBuilding maps using ggplot (Q3)\n\n\n\n\nQuestion 1:\nFor this lab three (3) datasets are needed.\n\nSpatial boundaries of continental USA states (1.1)\nBoundaries of Canada, Mexico and the United States (1.2)\nAll USA cites (1.3)\n\n\n1.1 Define a Projection\nFor this lab I want to calculate distances between features, therefore I need a projection that preserves distance at the scale of CONUS. For this, I will use the North America Equidistant Conic:\n\neqdc &lt;- '+proj=eqdc +lat_0=40 +lon_0=-96 +lat_1=20 +lat_2=60 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs'\n\nThis PROJ.4 string defines an Equidistant Conic projection with the following parameters:\n\n+proj=eqdc → Equidistant Conic projection\n+lat_0=40 → Latitude of the projection’s center (40°N)\n+lon_0=-96 → Central meridian (96°W)\n+lat_1=20 → First standard parallel (20°N)\n+lat_2=60 → Second standard parallel (60°N)\n+x_0=0 → False easting (0 meters)\n+y_0=0 → False northing (0 meters)\n+datum=NAD83 → Uses the North American Datum 1983 (NAD83)\n+units=m → Units are in meters\n+no_defs → No additional default parameters from PROJ’s database\n\nThis projection is commonly used for mapping large areas with an east-west extent, especially in North America, as it balances distortion well between the two standard parallels.\n\n\n1.2 - Get USA state boundaries\nIn R, USA boundaries are stored in the USAboundaries package.\n\nUSA state boundaries can be accessed with USAboundaries::us_states(resolution = \"low\"). Given the precision needed for this analysis, low resolution is sufficient.\nMake sure only the states in the continental United States (CONUS) are selected\nMake sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\n\nstate_boundary &lt;- us_states(resolution = 'low') %&gt;%\n  filter(!state_abbr %in% c('PR', 'HI', 'AK')) %&gt;%\n  st_transform(eqdc)\n\n\nst_crs(state_boundary)\n\nggplot(data = state_boundary) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n1.3 - Get country boundaries for Mexico, the United States of America, and Canada\nIn R, country boundaries are stored in the rnaturalearth package. - World boundaries can be accessed with rnaturalearth::countries110. - Make sure the data is in simple features (sf) format - Make sure to only have the countries wanted - Make sure the data is in a projected coordinate system suitable for distance measurements at the national scale (eqdc).\n\nna_boundaries &lt;- rnaturalearth::countries110 %&gt;%\n  filter(ADMIN %in% c('United States of America', 'Canada', 'Mexico')) %&gt;%\n  st_as_sf() %&gt;%\n  st_transform(eqdc)\n\n\nst_crs(na_boundaries)\n\nggplot(data = na_boundaries) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n1.4 - Get city locations from the CSV file\nThe process of finding, downloading and accessing data is the first step of every analysis.\nData was accessed at this (free) site and downloaded as a dataset into the data directory of this project.\nOnce downloaded, read it into my working session using and explored the dataset until comfortable with the information it contained.\nThe data has everything but it is not spatial. Convert the data.frame to a spatial object using st_as_sf and prescribing the coordinate variables and CRS.\nFinally, remove cities in states not wanted and make sure the data is in a projected coordinate system suitable for distance measurements at the national scale.\nCongratulations! here are three real-world, large datasets ready for analysis.\n\nus_cities &lt;- read_csv('data/uscities.csv') %&gt;%\n  st_as_sf(coords = c('lng', 'lat'), crs = 4326) %&gt;%\n  st_transform(eqdc) %&gt;%\n  filter(!state_id %in% c('PR', 'HI', 'AK'))\n\n\nst_crs(us_cities)\n\nggplot(data = us_cities) +\n  geom_sf() +\n  theme_void()\n\n\n\n\n\nQuestion 2:\nHere I focus on calculating the distance of each USA city to (1) the national border (2) the nearest state border (3) the Mexican border and (4) the Canadian border. The existing spatial geometries are manipulated to do this.\n\n2.1 - Distance to USA Border (coastline or national) (km)\n\nstate_boundary_u &lt;- state_boundary %&gt;%\n  st_union() %&gt;%\n  st_cast(to = 'MULTILINESTRING')\n\nus_cities_distance &lt;- us_cities %&gt;%\n  mutate(distance_usa = st_distance(us_cities, state_boundary_u)) %&gt;%\n  mutate(distance_usa = units::set_units(distance_usa, km)) %&gt;%\n  select(-c('city_ascii', 'source', 'military', 'incorporated', 'timezone', 'ranking', 'zips', 'id'))\n\nslice_max(us_cities_distance, n = 5, order_by = distance_usa) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(city, state_name, distance_usa) %&gt;%\n  flextable() %&gt;%\n  set_caption(\"5 Cities Furthest from a Coastal or International Border\") %&gt;%\n  theme_zebra()\n\ncitystate_namedistance_usaLudellKansas1,012.508 [km]DresdenKansas1,012.398 [km]HerndonKansas1,007.763 [km]Hill CityKansas1,005.140 [km]AtwoodKansas1,004.734 [km]\n\n\n\n\n\n2.2 - Distance to States (km)\nFor 2.2 we are interested in calculating the distance of each city to the nearest state boundary. To do this we need all states to act as single unit. Convert the USA state boundaries to a MULTILINESTRING geometry in which the state boundaries are preserved (not resolved). In addition to storing this distance data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\nstate_boundary_c &lt;- state_boundary %&gt;%\n  st_combine() %&gt;%\n  st_cast(to = 'MULTILINESTRING')\n\nus_cities_distance &lt;- us_cities_distance %&gt;%\n  mutate(distance_state = st_distance(us_cities_distance, state_boundary_c)) %&gt;%\n  mutate(distance_state = units::set_units(distance_state, km))\n\nslice_max(us_cities_distance, n = 5, order_by = distance_state) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(city, state_name, distance_state) %&gt;%\n  flextable() %&gt;%\n  set_caption(\"5 Cities Furthest from a State Boundary\") %&gt;%\n  theme_zebra()\n\ncitystate_namedistance_stateBriggsTexas309.4150 [km]LampasasTexas308.9216 [km]KempnerTexas302.5868 [km]BertramTexas302.5776 [km]Harker HeightsTexas298.8138 [km]\n\n\n\n\n\n2.3 - Distance to Mexico (km)\nFor 2.3 we are interested in calculating the distance of each city to the Mexican border. To do this we need to isolate Mexico from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from the Mexico border. Include only the city name, state, and distance.\n\nna_boundaries_mex &lt;- na_boundaries %&gt;%\n  filter(SOV_A3 == 'MEX') %&gt;%\n  st_cast(to = 'MULTILINESTRING')\n\nus_cities_distance &lt;- us_cities_distance %&gt;%\n  mutate(distance_mex = st_distance(us_cities_distance, na_boundaries_mex)) %&gt;%\n  mutate(distance_mex = units::set_units(distance_mex, km))\n  \nslice_max(us_cities_distance, n = 5, order_by = distance_mex) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(city, state_name, distance_mex) %&gt;%\n  flextable() %&gt;%\n  set_caption(\"5 Cities Furthest from the Mexico Border\") %&gt;%\n  theme_zebra()\n\ncitystate_namedistance_mexGrand IsleMaine3,282.825 [km]CaribouMaine3,250.330 [km]Presque IsleMaine3,234.570 [km]OakfieldMaine3,175.577 [km]Island FallsMaine3,162.285 [km]\n\n\n\n\n2.4 - Distance to Canada (km)\nFor 2.4 we are interested in calculating the distance of each city to the Canadian border. To do this we need to isolate Canada from the country objects. In addition to storing this data as part of the cities data.frame, produce a table (flextable) documenting the five cities farthest from a state border. Include only the city name, state, and distance.\n\nna_boundaries_can &lt;- na_boundaries %&gt;%\n  filter(SOV_A3 == 'CAN') %&gt;%\n  st_cast(to = 'MULTILINESTRING')\n\nus_cities_distance &lt;- us_cities_distance %&gt;%\n  mutate(distance_can = st_distance(us_cities_distance, na_boundaries_can)) %&gt;%\n  mutate(distance_can = units::set_units(distance_can, km))\n\nslice_max(us_cities_distance, n = 5, order_by = distance_can) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(city, state_name, distance_can) %&gt;%\n  flextable() %&gt;%\n  set_caption(\"5 Cities Furthest from the Canada Border\") %&gt;%\n  theme_zebra()\n\ncitystate_namedistance_canGuadalupe GuerraTexas2,206.455 [km]SandovalTexas2,205.641 [km]FrontonTexas2,204.794 [km]Fronton RanchettesTexas2,202.118 [km]EvergreenTexas2,202.020 [km]\n\n\n\n\n\n\nQuestion 3:\nIn this section I visualize the distance data calculated above. I use ggplot to make maps, ggrepl to label significant features, and gghighlight to emphasize important criteria.\n\n3.1 Data\nShow the 3 continents, CONUS outline, state boundaries, and 10 largest USA cities (by population) on a single map.\n\nus_cities_size &lt;- us_cities_distance %&gt;%\n  slice_max(n = 10, order_by = population)\n\nggplot() +\n  geom_sf(data = na_boundaries, linewidth = 0.75, color = 'black') +\n  geom_sf(data = state_boundary) +\n  geom_sf(data = us_cities_size, aes(color = population)) +\n  scale_color_viridis_c(option = 'turbo') +\n  geom_label_repel(data = us_cities_size,\n                   aes(x = st_coordinates(geometry)[,1],\n                       y = st_coordinates(geometry)[,2],\n                       label = city),\n                   size = 2) +\n  theme_void() +\n  theme(legend.position = 'none') +\n  labs(title = \"10 Largest U.S. Cities by Population\")\n\n\n\n\n\n\n\n\n\n\n\n3.2 City Distance from the Border\nCreate a map that colors USA cities by their distance from the national border. In addition, re-draw and label the 5 cities that are farthest from the border.\n\nus_cities_distance &lt;- us_cities_distance %&gt;%\n  mutate(distance_usa = as.numeric(distance_usa),\n         distance_state = as.numeric(distance_state),\n         distance_mex = as.numeric(distance_mex),\n         distance_can = as.numeric(distance_can))\n\nus_cities_far &lt;- us_cities_distance %&gt;%\n  slice_max(n = 5, order_by = distance_usa) %&gt;%\n  mutate(label = paste(city, state_id, sep = ','))\n\nggplot() +\n  geom_sf(data = state_boundary) +\n  geom_sf(data = us_cities_distance, aes(color = distance_usa), alpha = 0.7, size = 1) +\n  scale_color_viridis_c(option = 'rocket', name = 'Distance (km)') +\n  theme_void() +\n  labs(title = 'U.S. Cities and Distances from the Nearest Border') +\n  geom_sf(data = us_cities_far, color = 'black') +\n  geom_label_repel(data = us_cities_far,\n                   aes(x = st_coordinates(geometry)[,1],\n                       y = st_coordinates(geometry)[,2],\n                       label = label),\n                       size = 2.5)\n\n\n\n\n\n\n\n\n\n\n\n3.3 City Distance from Nearest State\nCreate a map that colors USA cities by their distance from the nearest state border. In addition, re-draw and label the 5 cities that are farthest from any border.\n\nus_cities_st_far &lt;- us_cities_distance %&gt;%\n  slice_max(n = 5, order_by = distance_state) %&gt;%\n  mutate(label = paste(city, state_id, sep = ','))\n\nstate_boundary_line &lt;- state_boundary %&gt;%\n  st_cast(to = 'MULTILINESTRING')\n\nggplot() +\n  geom_sf(data = us_cities_distance, aes(color = distance_state), alpha = 0.7, size = 1) +\n  scale_color_viridis_c(option = 'rocket', name = 'Distance (km)') +\n  theme_void() +\n  geom_sf(data = state_boundary_line, color = 'black', alpha = 0.8) +\n  labs(title = 'U.S. Cities and Distances from the Nearest State Border') +\n  geom_sf(data = us_cities_st_far, color = 'black') +\n  geom_label_repel(data = us_cities_st_far,\n                   aes(x = st_coordinates(geometry)[,1],\n                       y = st_coordinates(geometry)[,2],\n                       label = label),\n                       size = 2.5)\n\n\n\n\n\n\n\n\n\n\n\n3.4 Equidistance boundary from Mexico and Canada\nThis is slightly more challenging. Use gghighlight to identify the cities that are equal distance from the Canadian AND Mexican border \\(\\pm\\) 100 km.\nIn addition, label the five (5) most populous cites in this zone.\n\nus_cities_eqd &lt;- us_cities_distance %&gt;%\n  mutate(equidistant = abs(distance_mex - distance_can))\n\nus_cities_eqd_pop &lt;- us_cities_eqd %&gt;%\n  filter(equidistant &lt;= 100) %&gt;%\n  slice_max(n = 5, order_by = population) %&gt;%\n  mutate(label = paste(city, state_id, sep = ','))\n\nggplot() +\n  geom_sf(data = us_cities_eqd, aes(color = equidistant), alpha = 0.7, size = 1) +\n  scale_color_viridis_c(option = 'rocket', name = 'Distance (km)') +\n  gghighlight(equidistant &lt;= 100) +\n  theme_void() +\n  geom_sf(data = state_boundary_line, color = 'black', alpha = 0.8) +\n  labs(title = 'U.S. Cities Equidistant from Canada and Mexico +/- 100 km') +\n  geom_label_repel(data = us_cities_eqd_pop,\n                   aes(x = st_coordinates(geometry)[,1],\n                       y = st_coordinates(geometry)[,2],\n                       label = label),\n                       size = 2.5)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4:\n\nReal World Application\nRecently, Federal Agencies have claimed basic constitutional rights protected by the Fourth Amendment (protecting Americans from random and arbitrary stops and searches) do not apply fully at our borders (see Portland). For example, federal authorities do not need a warrant or suspicion of wrongdoing to justify conducting what courts have called a “routine search,” such as searching luggage or a vehicle. Specifically, federal regulations give U.S. Customs and Border Protection (CBP) authority to operate within 100 miles of any U.S. “external boundary”. Further information can be found at this ACLU article.\n\n\n4.1 Quantifing Border Zone\n\nHow many cities are in this 100 mile zone? (100 miles ~ 160 kilometers)\nHow many people live in a city within 100 miles of the border?\nWhat percentage of the total population is in this zone?\nDoes it match the ACLU estimate in the link above?\n\nReport this information as a table.\n\nus_cities_distance_border &lt;- us_cities_distance %&gt;%\n  filter(distance_usa &lt;= 161)\n\nn_cities &lt;- as.numeric(nrow(us_cities_distance_border))\ndanger_pop &lt;- sum(us_cities_distance_border$population)\ntotal_pop &lt;- sum(us_cities_distance$population)\npercent_pop &lt;- danger_pop/total_pop*100\n\nQ_table &lt;- tibble(\n  'Total Cities' = n_cities,\n  'Population within 100 miles' = danger_pop,\n  'Total U.S. Population' = total_pop,\n  '% in Danger Zone' = percent_pop,\n  'ACLU Estimate %' = '66.67')\n\nflextable(Q_table) %&gt;%\n  set_caption('Data for Cities within 100 miles of a US Border') %&gt;%\n  add_footer_lines('ACLU Estimate and calculated % from the lab data are within 2%. The ACLU used 2010 census data and the more current data matches that.')\n\nTotal CitiesPopulation within 100 milesTotal U.S. Population% in Danger ZoneACLU Estimate %13,220257,834,521396,228,55865.0721766.67ACLU Estimate and calculated % from the lab data are within 2%. The ACLU used 2010 census data and the more current data matches that.\n\n\n\n\n\n4.2 Mapping Border Zone\n\nMake a map highlighting the cites within the 100 mile zone using gghighlight.\nUse a color gradient from ‘orange’ to ‘darkred’.\nLabel the 10 most populous cities in the Danger Zone\n\n\nten_pop &lt;- us_cities_distance_border %&gt;%\n  slice_max(n = 10, order_by = population)\n\nggplot() +\n  geom_sf(data = us_cities_distance, color = 'black', alpha = 0.7, size = 0.5) +\n  geom_sf(data = us_cities_distance_border, aes(color = distance_usa)) +\n  scale_color_gradient(low = 'orange', high = 'darkred', name = 'Distance (km)') +\n  gghighlight(distance_usa &lt;= 161) +\n  theme_void() +\n  geom_sf(data = state_boundary_line, color = 'black', alpha = 0.8) +\n  labs(title = 'U.S. Cities within 100 miles (160 km) of the Border') +\n  geom_label_repel(data = ten_pop,\n                   aes(x = st_coordinates(geometry)[,1],\n                       y = st_coordinates(geometry)[,2],\n                       label = city),\n                       size = 2.5)\n\n\n\n\n\n\n\n\n\n\n\n4.3 : Instead of labeling the 10 most populous cities, label the most populous city in each state within the Danger Zone.\n\nstate_pop &lt;- us_cities_distance_border %&gt;%\n  group_by(state_name) %&gt;%\n  filter(population == max(population)) %&gt;%\n  ungroup()\n\nggplot() +\n  geom_sf(data = us_cities_distance, color = 'black', alpha = 0.7, size = 0.5) +\n  geom_sf(data = us_cities_distance_border, aes(color = distance_usa)) +\n  scale_color_gradient(low = 'orange', high = 'darkred', name = 'Distance (km)') +\n  gghighlight(distance_usa &lt;= 161) +\n  theme_void() +\n  geom_sf(data = state_boundary_line, color = 'black', alpha = 0.8) +\n  labs(title = 'U.S. Cities within 100 miles (160 km) of the Border') +\n  geom_label_repel(data = state_pop,\n                   max.overlaps = getOption('ggrepel.max.overlaps', default = 30),\n                   aes(x = st_coordinates(geometry)[,1],\n                       y = st_coordinates(geometry)[,2],\n                       label = city),\n                       size = 2.5)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESS 523c: Environmental Data Science Applications: Hydrology",
    "section": "",
    "text": "Poudre River"
  },
  {
    "objectID": "lab-01.html",
    "href": "lab-01.html",
    "title": "Lab 1: COVID-19 Trends",
    "section": "",
    "text": "COVID-19"
  },
  {
    "objectID": "lab-01.html#load-libraries",
    "href": "lab-01.html#load-libraries",
    "title": "Lab 1: COVID-19 Trends",
    "section": "Load Libraries",
    "text": "Load Libraries\n\nlibrary(tidyverse)\nlibrary(flextable)\nlibrary(zoo)\n\nIn this lab I practiced data wrangling and visualization skills using COVID-19 data curated by the New York Times. This is a large dataset measuring the cases and deaths per US county across the lifespan of COVID from its early beginnings to just beyond its peak in 2023. The data stored in daily cumulative counts is messy and is a great example of data that needs to be wrangled and cleaned before any analysis can be done."
  },
  {
    "objectID": "lab-01.html#data",
    "href": "lab-01.html#data",
    "title": "Lab 1: COVID-19 Trends",
    "section": "Data",
    "text": "Data\nThis data was used during the peak of the COVID-19 pandemic to create reports and data visualizations like this, and are archived on a GitHub repo here. A history of the importance can be found here.\nImagine it is Feb 1st, 2022. As a data scientist for the state of Colorado Department of Public Health, I have been tasked with giving a report to Governor Polis each morning about the most current COVID-19 conditions at the county level.\nAs it stands, the Colorado Department of Public Health maintains a watch list of counties that are being monitored for worsening COVID trends. There are six criteria used to place counties on the watch list:\n\nDoing fewer than 150 tests per 100,000 residents daily (over a 7-day average)\nMore than 100 new cases per 100,000 residents over the past 14 days…\n25 new cases per 100,000 residents and an 8% test positivity rate\n10% or greater increase in COVID-19 hospitalized patients over the past 3 days\nFewer than 20% of ICU beds available\nFewer than 25% ventilators available\n\nOf these 6 conditions, I am responsible for monitoring condition 2."
  },
  {
    "objectID": "lab-01.html#steps",
    "href": "lab-01.html#steps",
    "title": "Lab 1: COVID-19 Trends",
    "section": "Steps:",
    "text": "Steps:\n\nRead in the data from the NY-Times URL with read_csv using tidyverse. The data read from Github is considered the “raw data”. I leave “raw-data-raw” and generate meaningful subsets of data as I go.\nCreate an object called my.date and set it as “2022-02-01” - ensure this is a date object.\nCreate a object called my.state and set it to “Colorado”.\nStart by making a subset that limits the data to Colorado, and add a new column with the daily new cases by county. Do the same for new deaths.\nUsing your subset, generate (2) tables. The first should show the 5 counties with the most CUMULATIVE cases on your date of interest, and the second should show the 5 counties with the most NEW cases on that same date.\n\nRead in Data\n\ndat &lt;- read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\")\n\nInitial Objects\n\nmy.date &lt;- as.Date(\"2022-02-01\")\nmy.state &lt;- \"Colorado\"\n\nColorado Data\n\nco_dat &lt;- dat %&gt;%\n  filter(state == my.state) %&gt;%\n  group_by(county) %&gt;%\n  mutate(new_cases = cases - lag(cases, n = 1),\n         new_deaths = deaths - lag(deaths, n = 1)) %&gt;%\n  drop_na() %&gt;%\n  ungroup()\n\nData Tables\n\nco_dat_cum &lt;- co_dat %&gt;%\n  filter(date == my.date)\n\nslice_max(co_dat_cum, n = 5, order_by = cases) %&gt;%\n  select(county, state, cases, new_cases, deaths) %&gt;%\n  flextable() %&gt;%\n  set_caption(\"Top 5 Counties with Most Cumulative Cases\") %&gt;%\n  theme_zebra()\n\ncountystatecasesnew_casesdeathsEl PasoColorado170,6736301,518DenverColorado159,0223891,194ArapahoeColorado144,2554011,172AdamsColorado126,7683261,224JeffersonColorado113,2402911,219\n\nslice_max(co_dat_cum, n = 5, order_by = new_cases) %&gt;%\n  select(county, state, cases, new_cases, deaths) %&gt;%\n  flextable() %&gt;%\n  set_caption(\"Top 5 Counties with New Cases\") %&gt;%\n  theme_zebra()\n\ncountystatecasesnew_casesdeathsEl PasoColorado170,6736301,518ArapahoeColorado144,2554011,172DenverColorado159,0223891,194AdamsColorado126,7683261,224JeffersonColorado113,2402911,219"
  },
  {
    "objectID": "lab-01.html#steps-1",
    "href": "lab-01.html#steps-1",
    "title": "Lab 1: COVID-19 Trends",
    "section": "Steps:",
    "text": "Steps:\n\nCreated a five digit FIP variable and only keep columns that contain “NAME” or “2021”. Additionally all state level rows (e.g. COUNTY FIP == “000”) are removed.\n\n\npop_data &lt;- read_csv(pop_url)\n\npop_colo &lt;- pop_data %&gt;%\n  filter(STNAME == 'Colorado') %&gt;%\n  filter(COUNTY &gt; '000') %&gt;%\n  mutate(fips = paste0(STATE, COUNTY)) %&gt;%\n  select(fips, contains(c('NAME', '2021')))\n\n\nDescription of data after manipulation:\n\nI intentionally named the “fips” column “fips” to match with the COVID data for Colorado. I knew we would be merging these data tables together. The “pop_colo” data contains information about estimated population, deaths, births, and immigration statistics by county in Colorado. There are 19 columns, and 64 rows for the 64 counties. I like these functions for browsing the data: names(), glimpse(), head(), and summary(). The skim() function is sort of like the summary() function but needs the skimr package installed.\n\nnames(pop_colo)\nglimpse(pop_colo)\nstr(pop_colo)\nhead(pop_colo)\n\nlibrary(skimr)\nskim(pop_colo)"
  },
  {
    "objectID": "lab-01.html#steps-2",
    "href": "lab-01.html#steps-2",
    "title": "Lab 1: COVID-19 Trends",
    "section": "Steps:",
    "text": "Steps:\n\nI need to group/summarize the county level data to the state level and filter it to my four states of interest. Then calculate the number daily new cases and the 7-day rolling mean.\n\n\n\n\n\n\n\nRolling Averages\n\n\n\n\n\nThe rollmean function from the zoo package in R is used to compute the rolling (moving) mean of a numeric vector, matrix, or zoo/ts object.\nrollmean(x, k, fill = NA, align = \"center\", na.pad = FALSE)\n- x: Numeric vector, matrix, or time series.\n- k: Window size (number of observations).\n- fill: Values to pad missing results (default NA).\n- align: Position of the rolling window (“center”, “left”, “right”).\n- na.pad: If TRUE, pads missing values with NA.\n\n\nExamples\n\nRolling Mean on a Numeric Vector Since align = \"center\" by default, values at the start and end are dropped.\n\n\nlibrary(zoo)\n\n# Sample data\nx &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n\n# Rolling mean with a window size of 3\nrollmean(x, k = 3)\n\n[1] 2 3 4 5 6 7 8 9\n\n\n\nRolling Mean with Padding Missing values are filled at the start and end.\n\n\nrollmean(x, k = 3, fill = NA)\n\n [1] NA  2  3  4  5  6  7  8  9 NA\n\n\n\nAligning Left or Right The rolling mean is calculated with values aligned to the left or right\n\n\nrollmean(x, k = 3, fill = NA, align = \"left\")\n\n [1]  2  3  4  5  6  7  8  9 NA NA\n\nrollmean(x, k = 3, fill = NA, align = \"right\")\n\n [1] NA NA  2  3  4  5  6  7  8  9\n\n\n\n\n\n\n\nstates &lt;- c('Alabama', 'Colorado', 'New York', 'Ohio')\n\nstate_data &lt;- dat %&gt;%\n  group_by(state, county) %&gt;%\n  mutate(new_cases = cases - lag(cases, n = 1)) %&gt;%\n  ungroup() %&gt;%\n  group_by(state, date) %&gt;%\n  summarise(daily_cases = sum(cases),\n            daily_deaths = sum(deaths),\n            daily_new_cases = sum(new_cases)) %&gt;%\n  ungroup()\n\nstate_filter &lt;- state_data %&gt;%\n  filter(state %in% states) %&gt;%\n  select(-daily_new_cases) %&gt;%\n  mutate(new_cases = daily_cases - lag(daily_cases, n = 1)) %&gt;%\n  mutate(roll7_mean = zoo::rollmean(new_cases, k = 7, fill = NA, align = \"right\")) %&gt;%\n  filter(roll7_mean &gt;= 0)\n\nstate_filter$roll7_mean &lt;- round(state_filter$roll7_mean, 0)\n\n\nNow that I have the data modified, I can create a plot with the daily new cases and a 7-day rolling mean.\n\n\nggplot(data = state_filter) +\n  geom_line(mapping = aes(x = date, y = roll7_mean, color = roll7_mean), linewidth = 1) +\n  facet_wrap(. ~ state) +\n  theme_dark() +\n  scale_color_distiller(palette = 'Greens', name = 'Cases Gradient') +\n  labs(x = 'Date',\n       y = 'Number of Cases',\n       title = '7 Day Rolling Average Number of COVID-19 Cases') +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe story of raw case counts can be misleading. I now modify the data to show why.\n\n\npop_state &lt;- pop_data %&gt;%\n  filter(COUNTY &gt; '000',\n         STNAME %in% states) %&gt;%\n  mutate(fips = paste0(STATE, COUNTY)) %&gt;%\n  select(fips, contains(c('NAME', 'POPE', 'DEATHS'))) %&gt;%\n  group_by(STNAME) %&gt;%\n  summarise(POP2020 = sum(POPESTIMATE2020),\n            POP2021 = sum(POPESTIMATE2021),\n            POP2022 = sum(POPESTIMATE2022),\n            POP2023 = sum(POPESTIMATE2023)) %&gt;%\n  rename(state = 'STNAME')\n\nstate_join &lt;- full_join(pop_state, state_filter, by = 'state') %&gt;%\n  select(-roll7_mean) %&gt;%\n  mutate(year = lubridate::year(date)) %&gt;%\n  mutate(percap_cases = if_else(year == 2020, new_cases/POP2020,\n                                if_else(year == 2021, new_cases/POP2021,\n                                        if_else(year == 2022, new_cases/POP2022,\n                                                if_else(year == 2023, new_cases/POP2023, NA))))) %&gt;%\n  mutate(roll7_mean = zoo::rollmean(percap_cases, k = 7, fill = NA, align = \"right\"))\n\n\nAnd now plot the modified data.\n\n\nggplot(data = state_join) +\n  geom_line(mapping = aes(x = date, y = roll7_mean, color = state), linewidth = 1) +\n  theme_dark() +\n  scale_color_brewer(palette = \"Spectral\", name = \"State\") +\n  labs(x = 'Date',\n       y = 'per Capita Cases',\n       title = '7 Day per Capita Averages for COVID-19 Cases') +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nBriefly describe the influence scaling by population had on the analysis? Does it make some states look better? Some worse? How so? Scaling standardizes cases per capita, so the state that initially looks bad based on sheer numbers, New York, actually looks to have about the same number of cases per capita as the others looked at.New York looks bad because there is more population there, so there are more total cases."
  },
  {
    "objectID": "lab-01.html#data-preparation",
    "href": "lab-01.html#data-preparation",
    "title": "Lab 1: COVID-19 Trends",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nStart with the raw COVID dataset, and compute county level daily new cases and deaths. Then, join it to the census data in order to use population data in the model.\nThere was a strong seasonal component to the spread of COVID-19. To account for this, so add a new column to the data for year, month, and season which will be one of four values: “Spring” (Mar-May), “Summer” (Jun-Aug), “Fall” (Sep-Nov), or “Winter” (Dec - Jan) based on the computed Month.\nNext, group the data by state, year, and season and summarize the total population, new cases, and new deaths per grouping.\nGiven the case/death counts are not scaled by population, it is expected that each will exhibit a right skew behavior (you can confirm this with density plots, shapiro.test, or histrograms). Given an assumption of linear models is normality in the data, apply a log transformation to cases, deaths, and population to normalize them.\n\n\n\n\n\n\n\nNote\n\n\n\nWe know there are 0’s in the data (cases/deaths), so we can add 1 to the data before taking the log. As the log of 0 is undefined, adding 1 ensures that the log of 0 is -Inf.\n\nlog(0)\n\n[1] -Inf\n\n\n\n\n\nstate_dat &lt;- dat %&gt;%\n  group_by(state, county) %&gt;%\n  mutate(new_cases = cases - lag(cases, n = 1),\n         new_deaths = deaths - lag(deaths, n = 1)) %&gt;%\n  drop_na() %&gt;%\n  ungroup()\n\npop_state_cum &lt;- pop_data %&gt;%\n  filter(COUNTY &gt; '000') %&gt;%\n  mutate(fips = paste0(STATE, COUNTY))\n\npop_state_join &lt;- inner_join(pop_state_cum, state_dat, by = 'fips') %&gt;%\n  select(CTYNAME, STNAME, contains(c('POPES', 'DEATHS')), fips, date, county, state, cases, deaths, new_cases, new_deaths) %&gt;%\n  mutate(year = lubridate::year(date),\n         month = lubridate::month(date),\n         season = dplyr::case_when(month &gt;= 3 & month &lt;= 5 ~ 'Spring',\n                                   month &gt;= 6 & month &lt;= 8 ~ 'Summer',\n                                   month &gt;= 9 & month &lt;= 11 ~ 'Fall',\n                                   TRUE ~ 'Winter'))\n\npop_state_season &lt;- pop_state_join %&gt;%\n  group_by(state, year, season) %&gt;%\n  summarise(seasonal_cases = sum(new_cases),\n            seasonal_deaths = sum(new_deaths),\n            population = case_when(year == 2020 ~ first(POPESTIMATE2020),\n                                   year == 2021 ~ first(POPESTIMATE2021),\n                                   year == 2022 ~ first(POPESTIMATE2022),\n                                   year == 2023 ~ first(POPESTIMATE2023),\n                                   TRUE ~ NA_real_)) %&gt;%\n  distinct() %&gt;%\n  mutate(across(where(is.numeric), ~ifelse(. == 0, 1, .))) %&gt;%\n  mutate(log_seasonal_cases = log(seasonal_cases),\n         log_seasonal_deaths = log(seasonal_deaths),\n         log_population = log(population))"
  },
  {
    "objectID": "lab-01.html#model-building",
    "href": "lab-01.html#model-building",
    "title": "Lab 1: COVID-19 Trends",
    "section": "Model Building",
    "text": "Model Building\n\nOnce the data has been prepared, build a linear model (lm) to predict the log of cases using the log of deaths the log of population, and the season. Add an interaction term for population and deaths since the per capita realtionship is significant!\nOnce the model is built, summarize it (summary) and report the R-squared value and the p-value of the model. What does this mean for the value of its application?\n\n\ncases_lm &lt;- lm(log_seasonal_cases ~ log_seasonal_deaths + log_population + log_seasonal_deaths*log_population + season, data = pop_state_season)\nsummary(cases_lm)\n\n\nCall:\nlm(formula = log_seasonal_cases ~ log_seasonal_deaths + log_population + \n    log_seasonal_deaths * log_population + season, data = pop_state_season)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1437 -0.4495 -0.0083  0.4237  3.6495 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         6.84534    0.91781   7.458 3.99e-13 ***\nlog_seasonal_deaths                 0.59534    0.13901   4.283 2.22e-05 ***\nlog_population                     -0.08727    0.08206  -1.063   0.2881    \nseasonSpring                       -0.81867    0.08409  -9.736  &lt; 2e-16 ***\nseasonSummer                       -0.16527    0.09266  -1.784   0.0751 .  \nseasonWinter                        0.15947    0.08262   1.930   0.0542 .  \nlog_seasonal_deaths:log_population  0.01923    0.01239   1.552   0.1213    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6392 on 492 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.8102,    Adjusted R-squared:  0.8079 \nF-statistic:   350 on 6 and 492 DF,  p-value: &lt; 2.2e-16\n\n\nResults: r-squared = 0.8079, p-value &lt; 2.2e-16\nSince the p-value is so low, the seasonal deaths are dependent on population and seasonal cases. Therefore the model is useful could be used to predict deaths based on cases, season, and population."
  },
  {
    "objectID": "lab-03.html",
    "href": "lab-03.html",
    "title": "Lab 3: National Dam Inventory",
    "section": "",
    "text": "Dams"
  },
  {
    "objectID": "lab-03.html#step-1.1",
    "href": "lab-03.html#step-1.1",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 1.1",
    "text": "Step 1.1\nFirst, we need a spatial file of CONUS counties. For future area calculations we want these in an equal area projection (EPSG:5070).\nTo achieve this:\n\nget an sf object of US counties (AOI::aoi_get(state = “conus”, county = “all”))\ntransform the data to EPSG:5070\n\n\ncounties &lt;- AOI::aoi_get(state = \"conus\", county = \"all\") %&gt;%\n  st_transform(crs = 5070)\n\n\nggplot(data = counties) +\n  geom_sf()"
  },
  {
    "objectID": "lab-03.html#step-1.2",
    "href": "lab-03.html#step-1.2",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 1.2",
    "text": "Step 1.2\nFor triangle based tessellations I need point locations to serve as the “anchors”.\nTo achieve this:\n\nGenerate county centroids using st_centroid\nSince, I can only tessellate over a feature I need to combine or union the resulting 3,108 POINT features into a single MULTIPOINT feature\nSince these are point objects, the difference between union/combine is mute.\n\n\ncounties_cent &lt;- counties %&gt;%\n  st_centroid() %&gt;%\n  st_union()"
  },
  {
    "objectID": "lab-03.html#step-1.3",
    "href": "lab-03.html#step-1.3",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 1.3",
    "text": "Step 1.3\nTessellations/Coverage’s describe the extent of a region with geometric shapes, called tiles, with no overlaps or gaps. Tiles can range in size, shape, area and have different methods for being created. Some methods generate triangular tiles across a set of defined points (e.g. voroni and delauny triangulation). Others generate equal area tiles over a known extent (st_make_grid). For this lab, I will create surfaces of CONUS using using 4 methods, 2 based on an extent and 2 based on point anchors:\nTessellations:\n\nst_voronoi: creates voroni tessellation\nst_triangulate: triangulates set of points (not constrained)\n\nCoverage’s: - st_make_grid: Creates a square grid covering the geometry of an sf or sfc object\n\nst_make_grid(square = FALSE): Create a hexagonal grid covering the geometry of an sf or sfc object\nThe side of coverage tiles can be defined by a cell resolution or a specified number of cell in the X and Y direction\n\nFor this step:\n\nMake a voroni tessellation over your county centroids (MULTIPOINT)\nMake a triangulated tessellation over your county centroids (MULTIPOINT)\nMake a gridded coverage with n = 70, over your counties object\nMake a hexagonal coverage with n = 70, over your counties object\nIn addition to creating these 4 coverage’s, add an ID to each tile. To do this:\n\n\nadd a new column to each tessellation that spans from 1:n().\nRemember that ALL tessellation methods return an sfc GEOMETRYCOLLECTION, and to add attribute information - like the ID - I have to coerce the sfc list into an sf object (st_sf or st_as_sf)\n\nLast, ensure that our surfaces are topologically valid/simple:\n\nPass the surfaces through st_cast.\nCasting an object explicitly (e.g. st_cast(x, “POINT”)) changes a geometry\nIf no output type is specified (e.g. st_cast(x)) then the cast attempts to simplify the geometry\nIf this step is not performed, an error may occur unexpected “TopologyException”\n\n#Tesselations\n\nvor_counties &lt;- counties_cent %&gt;%\n  st_voronoi() %&gt;%\n  st_cast() %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = 1:n())\n\ntri_counties &lt;- counties_cent %&gt;%\n  st_triangulate() %&gt;%\n  st_cast() %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = 1:n())\n\nsqgrid_counties &lt;- counties_cent %&gt;%\n  st_make_grid(n = 70) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = 1:n())\n  \nhexgrid_counties &lt;- counties_cent %&gt;%\n  st_make_grid(square = FALSE, n = 70) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(id = 1:n())"
  },
  {
    "objectID": "lab-03.html#step-1.4",
    "href": "lab-03.html#step-1.4",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 1.4",
    "text": "Step 1.4\nIf tessellations are plotted, the triangulated surfaces can be seen to produce regions far beyond the boundaries of CONUS. Cut these boundaries to the CONUS border.\nTo do this, call on st_intersection, but first, a geometry of CONUS erves as the differencing feature. This is done by union-ing the existing county boundaries.\n\nu_counties &lt;- counties %&gt;%\n  st_union()"
  },
  {
    "objectID": "lab-03.html#step-1.5",
    "href": "lab-03.html#step-1.5",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 1.5",
    "text": "Step 1.5\nWith a single feature boundary, I must carefully consider the complexity of the geometry. Remember, the more points the geometry contains, the more computations needed for spatial predicates our differencing. For a task like this, I do not need a finely resolved coastal boarder.\nTo achieve this:\n\nSimplify the unioned border using the Visvalingam algorithm provided by rmapshaper::ms_simplify.\nChoose what percentage of vertices to retain using the keep argument and work to find the highest number that provides a shape you are comfortable with for the analysis.\nOnce happy with the simplification, use the mapview::npts function to report the number of points in the original object, and the number of points in the simplified object\nHow many points were removed? What are the consequences of doing this computationally?\nFinally, use the simplified object to crop the two triangulated tessellations with st_intersection:\n\n\nsimp_counties &lt;- u_counties %&gt;%\n  ms_simplify(keep = 0.01)\n\nggplot() +\n  geom_sf(data=simp_counties) +\n  geom_sf(data = u_counties) +\n  theme_void()\n\n\n\n\n\n\n\nmapview::npts(u_counties, by_feature = FALSE)\n\n[1] 11292\n\nmapview::npts(simp_counties, by_feature = FALSE)\n\n[1] 114\n\nvor_conus &lt;- st_intersection(vor_counties, simp_counties)\n\ntri_conus &lt;- st_intersection(tri_counties, simp_counties)\n\nsq_conus &lt;- st_intersection(sqgrid_counties, simp_counties)\n\nhex_conus &lt;- st_intersection(hexgrid_counties, simp_counties)"
  },
  {
    "objectID": "lab-03.html#step-1.6",
    "href": "lab-03.html#step-1.6",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 1.6",
    "text": "Step 1.6\nThe last step is to plot the tessellations. I don’t want to write out 5 ggplot codes (or mindlessly copy and paste 😄)\nInstead make a function that takes an sf object as arg1 and a character string as arg2 and returns a ggplot object showing arg1 titled with arg2.\nFor this function:\n\narg1 should take an sf object\narg2 should take a character string that will title the plot\nthe code should follow a standard ggplot practice where data is arg1, and the title is arg2\nThe function should also enforce the following:\na white fill a navy border a size of 0.2 `theme_void`` a caption that reports the number of features in arg1 You will need to paste character strings and variables together.\n\n\ntessplot &lt;- function(obj, char){\n  \n  ggplot(data = obj) +\n    geom_sf(fill = 'white', color = 'navy', size = 0.2) +\n    labs(title = char,\n         caption = paste('Number of points: ', mapview::npts(obj))) +\n    theme_void()\n\n}"
  },
  {
    "objectID": "lab-03.html#step-1.7",
    "href": "lab-03.html#step-1.7",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 1.7",
    "text": "Step 1.7\nUse your new function to plot each of your tessellated surfaces and the original county data (5 plots in total):\n\ntessplot(counties, char = 'CONUS Counties')\n\n\n\n\n\n\n\ntessplot(vor_conus, char = 'Voronoi Tesselation of CONUS Counties')\n\n\n\n\n\n\n\ntessplot(tri_conus, char = 'Triangulated Tesselation of CONUS Counties')\n\n\n\n\n\n\n\ntessplot(sq_conus, char = 'Square Coverage of CONUS Counties')\n\n\n\n\n\n\n\ntessplot(hex_conus, char = 'Hex Coverage of CONUS Counties')"
  },
  {
    "objectID": "lab-03.html#step-2.1",
    "href": "lab-03.html#step-2.1",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 2.1",
    "text": "Step 2.1\nFirst, I need a function that takes a sf object and a character string and returns a data.frame.\nFor this function:\n\nThe function name can be anything, arg1 should take an sf object, and arg2 should take a character string describing the object\ncalculate the area of arg1; convert the units to km2; and then drop the units\nNext, create a data.frame containing the following:\ntext from arg2 the number of features in arg1 the mean area of the features in arg1 (km2) the standard deviation of the features in arg1 the total area (km2) of arg1 Return this data.frame\n\n\ntessdf &lt;- function(obj, char) {\n  \n  mean_area &lt;- mean(st_area(obj)) %&gt;%\n    set_units(km^2) %&gt;%\n    drop_units() %&gt;%\n    round(digits = 1)\n  \n  sdev &lt;- sd(st_area(obj))/(1000000)\n  \n  tot_area &lt;- sum(st_area(obj)) %&gt;%\n    set_units(km^2) %&gt;%\n    drop_units()\n  \n  df_obj &lt;- data.frame(\n    'Description' = char,\n    'Elements' = mapview::npts(obj),\n    'Mean Area (km^2)' = mean_area,\n    'Std Dev. (km^2)' = sdev,\n    'Total Area (km^2)' = tot_area,\n    check.names = FALSE\n  )\n  \n  return(df_obj)\n}"
  },
  {
    "objectID": "lab-03.html#step-2.2",
    "href": "lab-03.html#step-2.2",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 2.2",
    "text": "Step 2.2\nUse the new function to summarize each of the tesselations and the original counties.\n\ndf_count &lt;- tessdf(obj = counties, char = 'counties')\n\ndf_vor &lt;- tessdf(obj = vor_conus, char = 'voronoi')\n\ndf_tri &lt;- tessdf(obj = tri_conus, char = 'triangulations')\n\ndf_sq &lt;- tessdf(obj = sq_conus, char = 'square grid')\n\ndf_hex &lt;- tessdf(obj = hex_conus, char = 'hex grid')"
  },
  {
    "objectID": "lab-03.html#step-2.3",
    "href": "lab-03.html#step-2.3",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 2.3",
    "text": "Step 2.3\nMultiple data.frame objects can bound row-wise with bind_rows into a single data.frame\n\ndf_total &lt;- bind_rows(df_count,\n                      df_vor,\n                      df_tri,\n                      df_sq,\n                      df_hex)"
  },
  {
    "objectID": "lab-03.html#step-2.4",
    "href": "lab-03.html#step-2.4",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 2.4",
    "text": "Step 2.4\nOnce the 5 summaries are bound (2 tessellations, 2 coverage’s, and the raw counties) print the data.frame as a nice table using knitr/kableExtra.\n\nkbl(df_total, caption = 'Information for Each Coverage') %&gt;%\n  kable_classic()\n\n\nInformation for Each Coverage\n\n\nDescription\nElements\nMean Area (km^2)\nStd Dev. (km^2)\nTotal Area (km^2)\n\n\n\n\ncounties\n383108\n2605.1\n3443.7121\n8096496\n\n\nvoronoi\n21641\n2601.7\n2908.2465\n8086096\n\n\ntriangulations\n25338\n1290.7\n1599.3152\n7999856\n\n\nsquare grid\n16678\n2433.9\n486.3631\n8063384\n\n\nhex grid\n16602\n3374.4\n726.0456\n8084951"
  },
  {
    "objectID": "lab-03.html#step-2.5",
    "href": "lab-03.html#step-2.5",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 2.5",
    "text": "Step 2.5\nComment on the traits of each tessellation. Be specific about how these traits might impact the results of a point-in-polygon analysis in the contexts of the modifiable areal unit problem and with respect computational requirements.\nThe Voronoi tesselations keeps the mean area of each county the most similar to the original counties object. The square grid has the lowest standard deviation for each county and the hex grid has the fewest elements and therefore the least calculation time involved.\nThe hex grid and Voronoi tesselation retain the most total area for similarity to the original counties object. ****"
  },
  {
    "objectID": "lab-03.html#step-3.1",
    "href": "lab-03.html#step-3.1",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 3.1",
    "text": "Step 3.1\nFind, download, and manage raw data: While the raw NID data is no longer easy to get with the transition of the USACE services to ESRI Features Services, the data is staged in the resources directory of this class.\n  - Read in the data\n  - Remove rows that don’t have location values (!is.na())\n  - Convert the data.frame to a sf object by defining the coordinates and CRS\n  - Transform the data to a CONUS AEA (EPSG:5070) projection - matching your tessellation\n  - Filter to include only those within your CONUS boundary\n\ndams &lt;- readr::read_csv('C:/Users/horre/Desktop/csu_523c/data/NID2019_U.csv')\n\nusa &lt;- AOI::aoi_get(state = \"conus\") %&gt;% \n  st_union() %&gt;% \n  st_transform(5070)\n\ndams2 &lt;- dams %&gt;% \n  filter(!is.na(LATITUDE)) %&gt;%\n  st_as_sf(coords = c(\"LONGITUDE\", \"LATITUDE\"), crs = 4236) %&gt;% \n  st_transform(5070) %&gt;% \n  st_filter(usa)"
  },
  {
    "objectID": "lab-03.html#step-3.2",
    "href": "lab-03.html#step-3.2",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 3.2",
    "text": "Step 3.2\nFollowing the in-class examples develop an efficient point-in-polygon function that takes:\n- points as arg1\n- polygons as arg2\n- The name of the id column as arg3\nThe function should make use of spatial and non-spatial joins, sf coercion and dplyr::count. The returned object should be input sf object with a column - n - counting the number of points in each tile.\n\npip_dams &lt;- function(polygon, points, var){\n  \n  coverage_join &lt;- st_join(polygon, points) %&gt;%\n    st_drop_geometry() %&gt;%\n    count(get(var)) %&gt;%\n    setNames(c(var, 'n')) %&gt;%\n    left_join(polygon, by = var) %&gt;%\n    st_as_sf()\n}"
  },
  {
    "objectID": "lab-03.html#step-3.3",
    "href": "lab-03.html#step-3.3",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 3.3",
    "text": "Step 3.3\nApply your point-in-polygon function to each of your five tessellated surfaces where:\n\nYour points are the dams\nYour polygons are the respective tessellation\nThe id column is the name of the id columns you defined.\n\n\ncounties_n &lt;- pip_dams(polygon = counties, points = dams2, var = 'fip_code')\n\nvor_n &lt;- pip_dams(polygon = vor_conus, points = dams2, var = 'id')\n\ntri_n &lt;- pip_dams(polygon = tri_conus, points = dams2, var = 'id')\n\nsq_n &lt;- pip_dams(polygon = sq_conus, points = dams2, var = 'id')\n\nhex_n &lt;- pip_dams(polygon = hex_conus, points = dams2, var = 'id')"
  },
  {
    "objectID": "lab-03.html#step-3.4",
    "href": "lab-03.html#step-3.4",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 3.4",
    "text": "Step 3.4\nI continue automating repetitive tasks through function creation. This time I make a new function that extends the previous plotting function.\nFor this function:\n- The name can be anything\n- arg1 should take an sf object, and arg2 should take a character string that will title the plot\n- The function should also enforce the following:\n\n  the fill aesthetic is driven by the count column n\n  the col is NA\n  the fill is scaled to a continuous viridis color ramp\n  theme_void\n  a caption that reports the number of dams in arg1 (e.g. sum(n))\nYou will need to paste character stings and variables together.\n\npip_plot &lt;- function(sf, char){\n  \n  ggplot(data = sf) +\n    geom_sf(aes(fill = n), color = NA) +\n    scale_fill_viridis_c() +\n    labs(title = char,\n         caption = paste('Total dams: ', sum(sf$n))) +\n    theme_void() +\n    theme(legend.position = 'bottom')\n}"
  },
  {
    "objectID": "lab-03.html#step-3.5",
    "href": "lab-03.html#step-3.5",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 3.5",
    "text": "Step 3.5\nApply the plotting function to each of the 5 tessellated surfaces with Point-in-Polygon counts:\n\npip_plot(sf = counties_n, char = 'Number of dams per County')\n\n\n\n\n\n\n\npip_plot(sf = vor_n, char = 'Number of dams per Voronoi County')\n\n\n\n\n\n\n\npip_plot(sf = tri_n, char = 'Number of dams per Triangulated County')\n\n\n\n\n\n\n\npip_plot(sf = sq_n, char = 'Number of dams per Square Grid')\n\n\n\n\n\n\n\npip_plot(sf = hex_n, char = 'Number of dams per Hex Grid')"
  },
  {
    "objectID": "lab-03.html#step-3.6",
    "href": "lab-03.html#step-3.6",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 3.6",
    "text": "Step 3.6\nComment on the influence of the tessellated surface in the visualization of point counts. How does this relate to the MAUP problem. Moving forward you will only use one tessellation, which will you chose and why?\nChanging the size or area that the number of dams are counted changes the mean or count of dams in a specific region. While there are the same number of total dams in each tesselated surface model, the voronoi and triangulation models decrease the number of individual units and change the area of those. The hex and square grids decrease the number of units and make each unit the same area.\nI will use the Voronoi Tesselation moving forward because it there is the total area and mean area is most similar to the original counties data set and has fewer units than the triangulation and is more representative to true counties than the square or hex grid."
  },
  {
    "objectID": "lab-03.html#step-4.1",
    "href": "lab-03.html#step-4.1",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 4.1",
    "text": "Step 4.1\n\nCreate point-in-polygon counts for at least 4 of the above dam purposes.\nUse grepl to filter the complete dataset to those with your chosen purpose.\ngrepl returns a boolean if a given pattern is matched in a string.\ngrepl is vectorized so can be used in dplyr::filter\n\nFor the analysis, I chose four of the codes. Then create a subset of dams that serve that purpose using dplyr::filter and grepl for each purpose.\nReasoning - I a choosing flood control (C), hydroelectric dams (H), navigation (N), and fish and wildlife dams (F). I chose these because I think they represent some common, and some unique purposes for dams, and hopefully do not overlap too much.\nFinally, use the point-in-polygon function to count each subset across your elected tessellation\n\npurpose &lt;- c('C', 'H', 'N', 'F')\n\ndams3 &lt;- dams2 %&gt;%\n  mutate(PURPOSES = strsplit(PURPOSES, \"\"))\n\ndams_C &lt;- dams3 %&gt;%\n  filter(grepl('C', PURPOSES) == TRUE)\n\ndams_H &lt;- dams3 %&gt;%\n  filter(grepl('H', PURPOSES) == TRUE)\n\ndams_N &lt;- dams3 %&gt;%\n  filter(grepl('N', PURPOSES) == TRUE)\n\ndams_F &lt;- dams3 %&gt;%\n  filter(grepl('F', PURPOSES) == TRUE)\n\n\npip_C &lt;- pip_dams(polygon = vor_conus, points = dams_C, var = 'id')\npip_H &lt;- pip_dams(polygon = vor_conus, points = dams_H, var = 'id')\npip_N &lt;- pip_dams(polygon = vor_conus, points = dams_N, var = 'id')\npip_F &lt;- pip_dams(polygon = vor_conus, points = dams_F, var = 'id')"
  },
  {
    "objectID": "lab-03.html#step-4.2",
    "href": "lab-03.html#step-4.2",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 4.2",
    "text": "Step 4.2\n\nNow use the plotting function from Q3 to map the counts\nUse gghighlight to only color the tiles where the count (n) is greater then the (mean + 1 standard deviation) of the set\nSince the plotting function returns a ggplot object already, the gghighlight call is added “+” directly to the function.\nThe result of this exploration is to highlight the areas of the country with the most\n\n\npip_plot(sf = pip_C, char = 'Flood Control Dams Across the US') +\n  gghighlight(n &gt; (mean(n)+sd(n)))\n\n\n\n\n\n\n\npip_plot(sf = pip_H, char = 'Hydroelectric Dams Across the US') +\n  gghighlight(n &gt; (mean(n)+sd(n)))\n\n\n\n\n\n\n\npip_plot(sf = pip_N, char = 'Navigation Dams Across the US') +\n  gghighlight(n &gt; (mean(n)+sd(n)))\n\n\n\n\n\n\n\npip_plot(sf = pip_F, char = 'Fish and Wildlife Dams Across the US') +\n  gghighlight(n &gt; (mean(n)+sd(n)))"
  },
  {
    "objectID": "lab-03.html#step-4.3",
    "href": "lab-03.html#step-4.3",
    "title": "Lab 3: National Dam Inventory",
    "section": "Step 4.3",
    "text": "Step 4.3\nComment of geographic distribution of dams you found. Does it make sense? How might the tessellation you chose impact your findings? How does the distribution of dams coincide with other geographic factors such as river systems, climate, etc.?\nFlood control dams appear mostly through the center of the country, near the Mississippi River which makes sense to me. There are select other spots, but the highest concentrations lie along this basin. Hydroelectric dams occur on the West coast, and in the Northeast. This also makes sense to me. Navigation dams mostly occur east of the Mississippi as well. I guess this makes sense as most large and navigable rivers occur in this area? Fish and wildlife dams occur in the arid west where more environmental regulation is necessary to ensure environmental sustainability. There is a high concentration in Alabama it appears as well."
  },
  {
    "objectID": "lab-05.html",
    "href": "lab-05.html",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "",
    "text": "#Data Manipulation\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(dataRetrieval)\nlibrary(AOI)\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\nlibrary(baguette)\nlibrary(ranger)\nlibrary(xgboost)\n\n#Data Visualization\nlibrary(ggpubr)\nlibrary(gghighlight)\nlibrary(ggrepel)\nlibrary(ggthemes)\nlibrary(flextable)"
  },
  {
    "objectID": "lab-05.html#exploratory-data-analysis",
    "href": "lab-05.html#exploratory-data-analysis",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFirst, lets make a map of the sites. Use the borders() ggplot function to add state boundaries to the map and initially color the points by the mean flow (q_mean) at each site.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()"
  },
  {
    "objectID": "lab-05.html#model-preparation",
    "href": "lab-05.html#model-preparation",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Model Preparation",
    "text": "Model Preparation\nLook at the relationship between aridity, rainfall and mean flow. Check the correlation between these three variables. Drop NAs and only view the 3 columns of interest.\n\ncamels_cor &lt;- camels %&gt;%\n  select(aridity, p_mean, q_mean) %&gt;% \n  drop_na() %&gt;% \n  cor()\n\ncamels_cor\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\nAs expected, there is a strong correlation between rainfall and mean flow, and an inverse correlation between aridity and rainfall. While both are high, we are going see if we can build a model to predict mean flow using aridity and rainfall."
  },
  {
    "objectID": "lab-05.html#visual-eda",
    "href": "lab-05.html#visual-eda",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Visual EDA",
    "text": "Visual EDA\n\nStart by looking that the 3 dimensions (variables) of this data. Start with a XY plot of aridity and rainfall. Use the scale_color_viridis_c() function to color the points by the q_mean column. This scale function maps the color of the points to the values in the q_mean column along the viridis continuous (c) palette. Because a scale_color_* function is applied, it maps to the known color aesthetic in the plot.\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall (mm/day)\",\n       color = \"Mean Flow (mm/day)\") +\n  ylim(0, 10)\n\n\n\n\n\n\n\n\nIt looks like there is a relationship between rainfall, aridity, and runoff but it looks like an exponential decay function and is not linear.\nTo test a transformation, log transform the x and y axes using the scale_x_log10() and scale_y_log10() functions:\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow (mm/day)\")\n\n\n\n\n\n\n\n\nGreat! There is a log-log relationship between aridity and rainfall and it provides a more linear relationship. This is a common relationship in hydrology and is often used to estimate rainfall in ungauged basins. However, once the data is transformed, the lack of spread in the streamflow data is quite evident with high mean flow values being compressed to the low end of aridity/high end of rainfall.\nTo address this, visualize how a log transform may benefit the q_mean data as well. Since the data is represented by color, rather then an axis, use the trans (transform) argument in the scale_color_viridis_c() function to log transform the color scale.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n\n\n\n\n\n\n\nExcellent! Treating these three right skewed variables as log transformed, an evenly spread relationship between aridity, rainfall, and mean flow can be seen. This is a good sign for building a model to predict mean flow using aridity and rainfall. ****"
  },
  {
    "objectID": "lab-05.html#start-by-splitting-the-data",
    "href": "lab-05.html#start-by-splitting-the-data",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Start by splitting the data",
    "text": "Start by splitting the data\n\nSet a seed for reproduceabilty, then transform the q_mean column to a log scale. It is error prone to apply transformations to the outcome variable within a recipe, so, we’ll do it a priori.\nOnce set, split the data into a training and testing set. Use 80% of the data for training and 20% for testing with no stratification.\nAdditionally, create a 10-fold cross validation dataset so as to evaluate multi-model setups.\n\nset.seed(9257)\n# Bad form to perform simple transformations on the outcome variable within a recipe. So, we'll do it here.\ncamels_mod &lt;- camels %&gt;% \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels_mod, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10) #cv = cross-validation, v = # folds"
  },
  {
    "objectID": "lab-05.html#preprocessor-recipe",
    "href": "lab-05.html#preprocessor-recipe",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Preprocessor: recipe",
    "text": "Preprocessor: recipe\nIn lecture, we have focused on using formulas as a workflow pre-processor. Separately we have used the recipe function to define a series of data pre-processing steps. Here, we are going to use the recipe function to define a series of data pre-processing steps.\nWe learned quite a lot about the data in the visual EDA. We know that the q_mean, aridity and p_mean columns are right skewed and can be helped by log transformations. We also know that the relationship between aridity and p_mean is non-linear and can be helped by adding an interaction term to the model. To implement these, lets build a recipe!\n\n# Create a recipe to preprocess the data\nrec &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) %&gt;%\n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())"
  },
  {
    "objectID": "lab-05.html#naive-base-lm-approach",
    "href": "lab-05.html#naive-base-lm-approach",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Naive base lm approach:",
    "text": "Naive base lm approach:\nOk, to start, lets do what we are comfortable with … fitting a linear model to the data. First, we use prep and bake on the training data to apply the recipe. Then, we fit a linear model to the data.\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) %&gt;%\n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.80103 -0.21464 -0.02143  0.21232  2.54675 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.59480    0.16214  -9.836  &lt; 2e-16 ***\naridity        -0.86401    0.15696  -5.505 5.76e-08 ***\np_mean          1.33565    0.15375   8.687  &lt; 2e-16 ***\naridity:p_mean  0.06208    0.06945   0.894    0.372    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5612 on 531 degrees of freedom\nMultiple R-squared:  0.7622,    Adjusted R-squared:  0.7609 \nF-statistic: 567.4 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.80103 -0.21464 -0.02143  0.21232  2.54675 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.59480    0.16214  -9.836  &lt; 2e-16 ***\naridity          -0.86401    0.15696  -5.505 5.76e-08 ***\np_mean            1.33565    0.15375   8.687  &lt; 2e-16 ***\naridity_x_p_mean  0.06208    0.06945   0.894    0.372    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5612 on 531 degrees of freedom\nMultiple R-squared:  0.7622,    Adjusted R-squared:  0.7609 \nF-statistic: 567.4 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n# They are EQUAL."
  },
  {
    "objectID": "lab-05.html#where-things-get-a-little-messy",
    "href": "lab-05.html#where-things-get-a-little-messy",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Where things get a little messy…",
    "text": "Where things get a little messy…\nOk so now we have our trained model lm_base and want to validate it on the test data.\nRemember a model's ability to predict on new data is the most important part of the modeling process. It really doesnt matter how well it does on data it has already seen!\nWe have to be careful about how we do this with the base R approach: INCORRECT PREDICTIONS\n\nnrow(camels_test)\nnrow(camels_train)\nbroom::augment(lm_base, data = camels_test)\n\ncamels_test$p2 = predict(lm_base, newdata = camels_test)\n#View Data\nggplot(camels_test, aes(x = p2, y = logQmean)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = TRUE, size =1) +\n  geom_abline(color = \"red\", size = 1) + \n  labs(title = \"Linear Model Using `predict()`\",\n       x = \"Predicted Log Mean Flow\",\n       y = \"Observed Log Mean Flow\") + \n  theme_linedraw()"
  },
  {
    "objectID": "lab-05.html#correct-version-prep---bake---predict",
    "href": "lab-05.html#correct-version-prep---bake---predict",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Correct version: prep -> bake -> predict",
    "text": "Correct version: prep -&gt; bake -&gt; predict\n\nTo correctly evaluate the model on the test data, we need to apply the same pre-processing steps to the test data that we applied to the training data. We can do this using the prep and bake functions with the recipe object. This ensures the test data is transformed in the same way as the training data before making predictions.\n\ntest_data &lt;- bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)"
  },
  {
    "objectID": "lab-05.html#model-evaluation-statistical-and-visual",
    "href": "lab-05.html#model-evaluation-statistical-and-visual",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\n\nNow that we have the predicted values, we can evaluate the model using the metrics function from the yardstick package. This function calculates common regression metrics such as RMSE, R-squared, and MAE between the observed and predicted values.\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.625\n2 rsq     standard       0.797\n3 mae     standard       0.383\n\n\n\nmod1 &lt;- ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Base Model\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\nSo that was a bit burdensome, is really error prone (fragile), and is worthless if we wanted to test a different algorithm… lets look at a better approach!"
  },
  {
    "objectID": "lab-05.html#using-a-workflow-instead",
    "href": "lab-05.html#using-a-workflow-instead",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Using a workflow instead",
    "text": "Using a workflow instead\n\ntidymodels provides a framework for building and evaluating models using a consistent and modular workflow. The workflows package allows you to define a series of modeling steps, including data pre-processing, model fitting in a single object. This makes it easier to experiment with different models, compare performance, and ensure reproducibility.\nworkflows are built from a model, a pre-processor, and an execution. Here, we are going to use the linear_reg function to define a linear regression model, set the engine to lm, and the mode to regression. We then add our recipe to the workflow, fit the model to the training data, and extract the model coefficients.\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.5948018  0.1621433 -9.8357545 4.391230e-21\naridity          -0.8640099  0.1569591 -5.5046826 5.764840e-08\np_mean            1.3356457  0.1537493  8.6871645 4.607667e-17\naridity_x_p_mean  0.0620750  0.0694540  0.8937571 3.718568e-01\n\n\nLets ensure we replicated the results from the lm_base model. How do they look to you? They are the same, so the models are the same.\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.5948018  0.1621433 -9.8357545 4.391230e-21\naridity        -0.8640099  0.1569591 -5.5046826 5.764840e-08\np_mean          1.3356457  0.1537493  8.6871645 4.607667e-17\naridity:p_mean  0.0620750  0.0694540  0.8937571 3.718568e-01"
  },
  {
    "objectID": "lab-05.html#making-predictions",
    "href": "lab-05.html#making-predictions",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Making Predictions",
    "text": "Making Predictions\nNow that lm_wf is a workflow, data is not embedded in the model, we can use augment with the new_data argument to make predictions on the test data.\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61"
  },
  {
    "objectID": "lab-05.html#model-evaluation-statistical-vs.-visual",
    "href": "lab-05.html#model-evaluation-statistical-vs.-visual",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical vs. visual",
    "text": "Model Evaluation: statistical vs. visual\n\nAs with EDA, applying for graphical and statistical evaluation of the model is a key Here, we use the metrics function to extract the default metrics (rmse, rsq, mae) between the observed and predicted mean stream flow values.\nWe then create a scatter plot of the observed vs predicted values, colored by aridity, to visualize the model performance.\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.625\n2 rsq     standard       0.797\n3 mae     standard       0.383\n\n\n\nmod2 &lt;- ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(title = \"Linear Model: Workflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\nggarrange(mod1, mod2,\n          ncol = 2,\n          nrow = 1,\n          common.legend = FALSE)\n\n\n\n\n\n\n\n\nThere are differences in the two plots because the Aridity scale is different. Why is it different in workflows vs. the base lm model approach?"
  },
  {
    "objectID": "lab-05.html#switch-it-up",
    "href": "lab-05.html#switch-it-up",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Switch it up!",
    "text": "Switch it up!\n\nThe real power of this approach is that we can easily switch out the models/recipes and see how it performs. Here, we are going to instead use a random forest model to predict mean stream flow. We define a random forest model using the rand_forest function, set the engine to ranger, and the mode to regression. We then add the recipe, fit the model, and evaluate the skill.\n\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train)"
  },
  {
    "objectID": "lab-05.html#predictions",
    "href": "lab-05.html#predictions",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Predictions",
    "text": "Predictions\n\n\nMake predictions on the test data using the augment function and the new_data argument.\n\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60"
  },
  {
    "objectID": "lab-05.html#model-evaluation-statistical-and-visual-1",
    "href": "lab-05.html#model-evaluation-statistical-and-visual-1",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\n\nEvaluate the model using the metrics function and create a scatter plot of the observed vs predicted values, colored by aridity.\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.593\n2 rsq     standard       0.797\n3 mae     standard       0.344\n\n\n\nmod3 &lt;- ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(title = \"Random Forest Model: Workflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\nggarrange(mod1, mod2, mod3,\n          ncol = 3,\n          nrow = 1,\n          common.legend = FALSE)\n\n\n\n\n\n\n\n\nAwesome! We just set up a completely new model and were able to utilize all of the things we had done for the linear model. This is the power of the tidymodels framework!\nThat said, we still can reduce some to the repetition. Further, we are not really able to compare these models to one another."
  },
  {
    "objectID": "lab-05.html#a-workflowset-approach",
    "href": "lab-05.html#a-workflowset-approach",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "A workflowset approach",
    "text": "A workflowset approach\n\nworkflow_set is a powerful tool for comparing multiple models on the same data. It allows you to define a set of workflows, fit them to the same data, and evaluate their performance using a common metric. Here, we are going to create a workflow_set object with the linear regression and random forest models, fit them to the training data, and compare their performance using the autoplot and rank_results functions.\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.554  0.0322    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.776  0.0137    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.557  0.0327    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0167    10 recipe       line…     2\n\n\nOverall it seems the random forest model is outperforming the linear model. This is not surprising given the non-linear relationship between the predictors and the outcome :)"
  },
  {
    "objectID": "lab-05.html#evaluation",
    "href": "lab-05.html#evaluation",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Evaluation",
    "text": "Evaluation\n\nAs a last step, lets evaluate the Random Forest model’s performance in predicting stream flow using the vip, augment, and ggplot2. We’ll start by computing variable importance (vip::vip()) to understand which predictors most influence the model.\nNext, we’ll apply the trained model (final) to the test data set using augment to append predictions to the test data.\nModel performance is then assessed using metrics(), comparing the actual (logQmean) and predicted (.pred) log-transformed mean stream flow values.\nFinally, a scatter plot is generated, visualizing the observed vs. predicted values, color-coded by aridity. The plot includes a 1:1 reference line (geom_abline()) to indicate perfect predictions and uses the viridis color scale to improve readability.\n\n# VIP\nvip::vip(final)\n\np_mean is the most important variable in predicting mean streamflow. This makes sense because streams are driven by precipitation (and groundwater).\n\n## Prediction\nrf_data &lt;- augment(final, new_data = camels_test)\n\n## Evaluation\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  geom_smooth(method = \"lm\", col = 'red', lty = 2, se = FALSE) +\n  theme_linedraw() + \n  labs(title = \"Random Forest Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  },
  {
    "objectID": "lab-05.html#comparing-models",
    "href": "lab-05.html#comparing-models",
    "title": "Lab 5: Machine Learning in Hydrology",
    "section": "Comparing Models",
    "text": "Comparing Models\n\nfull_wf &lt;- workflow_set(list(rec), list(lm_model, rf_model, boost_model, neural_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\nautoplot(full_wf)\n\n\n\n\n\n\n\n\n\nrank_results(full_wf, rank_metric = 'rsq', select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.545  0.0358    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.786  0.0155    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.554  0.0340    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.775  0.0153    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.557  0.0327    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0167    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.576  0.0377    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.761  0.0169    10 recipe       boos…     4\n\n\nWhich Model to choose? Based on the models, the random forest and neural network models perform the best with random forest barely outperforming neural networks with slightly less error in the rmse and rsq. The boosted model performed the worst but all models are very close.\n\nmod4 &lt;- ggplot(boost_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(title = \"XG Boost Model: Workflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\nmod5 &lt;- ggplot(neural_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(title = \"Neural Network Model: Workflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\nggarrange(mod2, mod3, mod4, mod5,\n          ncol = 2,\n          nrow = 2,\n          common.legend = FALSE)"
  }
]